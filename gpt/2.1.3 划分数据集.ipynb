{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7896089c-4214-4502-acd0-d996cdd6f102",
   "metadata": {},
   "source": [
    "### 2.1.3 划分数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43969ccf-7c95-4d3a-b74a-fd954ddb0f3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[733], [7599], [4788], [3857], [833], [3300], [3198], [8024], [4684], [2899], [756], [2359], [3845], [3771], [3862], [733, 7599], [7599, 4788], [4788, 3857], [3857, 833], [833, 3300], [3300, 3198], [3198, 8024], [8024, 4684], [4684, 2899], [2899, 756], [756, 2359], [2359, 3845], [3845, 3771], [3771, 3862], [733, 7599, 4788], [7599, 4788, 3857], [4788, 3857, 833], [3857, 833, 3300], [833, 3300, 3198], [3300, 3198, 8024], [3198, 8024, 4684], [8024, 4684, 2899], [4684, 2899, 756], [2899, 756, 2359], [756, 2359, 3845], [2359, 3845, 3771], [3845, 3771, 3862], [733, 7599, 4788, 3857], [7599, 4788, 3857, 833], [4788, 3857, 833, 3300], [3857, 833, 3300, 3198], [833, 3300, 3198, 8024], [3300, 3198, 8024, 4684], [3198, 8024, 4684, 2899], [8024, 4684, 2899, 756], [4684, 2899, 756, 2359], [2899, 756, 2359, 3845], [756, 2359, 3845, 3771], [2359, 3845, 3771, 3862], [733, 7599, 4788, 3857, 833], [7599, 4788, 3857, 833, 3300], [4788, 3857, 833, 3300, 3198], [3857, 833, 3300, 3198, 8024], [833, 3300, 3198, 8024, 4684], [3300, 3198, 8024, 4684, 2899], [3198, 8024, 4684, 2899, 756], [8024, 4684, 2899, 756, 2359], [4684, 2899, 756, 2359, 3845], [2899, 756, 2359, 3845, 3771], [756, 2359, 3845, 3771, 3862], [733, 7599, 4788, 3857, 833, 3300], [7599, 4788, 3857, 833, 3300, 3198], [4788, 3857, 833, 3300, 3198, 8024], [3857, 833, 3300, 3198, 8024, 4684], [833, 3300, 3198, 8024, 4684, 2899], [3300, 3198, 8024, 4684, 2899, 756], [3198, 8024, 4684, 2899, 756, 2359], [8024, 4684, 2899, 756, 2359, 3845], [4684, 2899, 756, 2359, 3845, 3771], [2899, 756, 2359, 3845, 3771, 3862], [733, 7599, 4788, 3857, 833, 3300, 3198], [7599, 4788, 3857, 833, 3300, 3198, 8024], [4788, 3857, 833, 3300, 3198, 8024, 4684], [3857, 833, 3300, 3198, 8024, 4684, 2899], [833, 3300, 3198, 8024, 4684, 2899, 756], [3300, 3198, 8024, 4684, 2899, 756, 2359], [3198, 8024, 4684, 2899, 756, 2359, 3845], [8024, 4684, 2899, 756, 2359, 3845, 3771], [4684, 2899, 756, 2359, 3845, 3771, 3862], [733, 7599, 4788, 3857, 833, 3300, 3198, 8024], [7599, 4788, 3857, 833, 3300, 3198, 8024, 4684], [4788, 3857, 833, 3300, 3198, 8024, 4684, 2899], [3857, 833, 3300, 3198, 8024, 4684, 2899, 756], [833, 3300, 3198, 8024, 4684, 2899, 756, 2359], [3300, 3198, 8024, 4684, 2899, 756, 2359, 3845], [3198, 8024, 4684, 2899, 756, 2359, 3845, 3771], [8024, 4684, 2899, 756, 2359, 3845, 3771, 3862]]\n",
      "[7599, 4788, 3857, 833, 3300, 3198, 8024, 4684, 2899, 756, 2359, 3845, 3771, 3862, 511, 4788, 3857, 833, 3300, 3198, 8024, 4684, 2899, 756, 2359, 3845, 3771, 3862, 511, 3857, 833, 3300, 3198, 8024, 4684, 2899, 756, 2359, 3845, 3771, 3862, 511, 833, 3300, 3198, 8024, 4684, 2899, 756, 2359, 3845, 3771, 3862, 511, 3300, 3198, 8024, 4684, 2899, 756, 2359, 3845, 3771, 3862, 511, 3198, 8024, 4684, 2899, 756, 2359, 3845, 3771, 3862, 511, 8024, 4684, 2899, 756, 2359, 3845, 3771, 3862, 511, 4684, 2899, 756, 2359, 3845, 3771, 3862, 511]\n",
      "torch.Size([92, 8]) torch.Size([92])\n",
      "tensor([[ 733,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [7599,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [4788,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [3857,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [ 833,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [3300,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [3198,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [8024,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [4684,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [2899,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [ 756,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [2359,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [3845,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [3771,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [3862,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [ 733, 7599,    0,    0,    0,    0,    0,    0],\n",
      "        [7599, 4788,    0,    0,    0,    0,    0,    0],\n",
      "        [4788, 3857,    0,    0,    0,    0,    0,    0],\n",
      "        [3857,  833,    0,    0,    0,    0,    0,    0],\n",
      "        [ 833, 3300,    0,    0,    0,    0,    0,    0],\n",
      "        [3300, 3198,    0,    0,    0,    0,    0,    0],\n",
      "        [3198, 8024,    0,    0,    0,    0,    0,    0],\n",
      "        [8024, 4684,    0,    0,    0,    0,    0,    0],\n",
      "        [4684, 2899,    0,    0,    0,    0,    0,    0],\n",
      "        [2899,  756,    0,    0,    0,    0,    0,    0],\n",
      "        [ 756, 2359,    0,    0,    0,    0,    0,    0],\n",
      "        [2359, 3845,    0,    0,    0,    0,    0,    0],\n",
      "        [3845, 3771,    0,    0,    0,    0,    0,    0],\n",
      "        [3771, 3862,    0,    0,    0,    0,    0,    0],\n",
      "        [ 733, 7599, 4788,    0,    0,    0,    0,    0],\n",
      "        [7599, 4788, 3857,    0,    0,    0,    0,    0],\n",
      "        [4788, 3857,  833,    0,    0,    0,    0,    0],\n",
      "        [3857,  833, 3300,    0,    0,    0,    0,    0],\n",
      "        [ 833, 3300, 3198,    0,    0,    0,    0,    0],\n",
      "        [3300, 3198, 8024,    0,    0,    0,    0,    0],\n",
      "        [3198, 8024, 4684,    0,    0,    0,    0,    0],\n",
      "        [8024, 4684, 2899,    0,    0,    0,    0,    0],\n",
      "        [4684, 2899,  756,    0,    0,    0,    0,    0],\n",
      "        [2899,  756, 2359,    0,    0,    0,    0,    0],\n",
      "        [ 756, 2359, 3845,    0,    0,    0,    0,    0],\n",
      "        [2359, 3845, 3771,    0,    0,    0,    0,    0],\n",
      "        [3845, 3771, 3862,    0,    0,    0,    0,    0],\n",
      "        [ 733, 7599, 4788, 3857,    0,    0,    0,    0],\n",
      "        [7599, 4788, 3857,  833,    0,    0,    0,    0],\n",
      "        [4788, 3857,  833, 3300,    0,    0,    0,    0],\n",
      "        [3857,  833, 3300, 3198,    0,    0,    0,    0],\n",
      "        [ 833, 3300, 3198, 8024,    0,    0,    0,    0],\n",
      "        [3300, 3198, 8024, 4684,    0,    0,    0,    0],\n",
      "        [3198, 8024, 4684, 2899,    0,    0,    0,    0],\n",
      "        [8024, 4684, 2899,  756,    0,    0,    0,    0],\n",
      "        [4684, 2899,  756, 2359,    0,    0,    0,    0],\n",
      "        [2899,  756, 2359, 3845,    0,    0,    0,    0],\n",
      "        [ 756, 2359, 3845, 3771,    0,    0,    0,    0],\n",
      "        [2359, 3845, 3771, 3862,    0,    0,    0,    0],\n",
      "        [ 733, 7599, 4788, 3857,  833,    0,    0,    0],\n",
      "        [7599, 4788, 3857,  833, 3300,    0,    0,    0],\n",
      "        [4788, 3857,  833, 3300, 3198,    0,    0,    0],\n",
      "        [3857,  833, 3300, 3198, 8024,    0,    0,    0],\n",
      "        [ 833, 3300, 3198, 8024, 4684,    0,    0,    0],\n",
      "        [3300, 3198, 8024, 4684, 2899,    0,    0,    0],\n",
      "        [3198, 8024, 4684, 2899,  756,    0,    0,    0],\n",
      "        [8024, 4684, 2899,  756, 2359,    0,    0,    0],\n",
      "        [4684, 2899,  756, 2359, 3845,    0,    0,    0],\n",
      "        [2899,  756, 2359, 3845, 3771,    0,    0,    0],\n",
      "        [ 756, 2359, 3845, 3771, 3862,    0,    0,    0],\n",
      "        [ 733, 7599, 4788, 3857,  833, 3300,    0,    0],\n",
      "        [7599, 4788, 3857,  833, 3300, 3198,    0,    0],\n",
      "        [4788, 3857,  833, 3300, 3198, 8024,    0,    0],\n",
      "        [3857,  833, 3300, 3198, 8024, 4684,    0,    0],\n",
      "        [ 833, 3300, 3198, 8024, 4684, 2899,    0,    0],\n",
      "        [3300, 3198, 8024, 4684, 2899,  756,    0,    0],\n",
      "        [3198, 8024, 4684, 2899,  756, 2359,    0,    0],\n",
      "        [8024, 4684, 2899,  756, 2359, 3845,    0,    0],\n",
      "        [4684, 2899,  756, 2359, 3845, 3771,    0,    0],\n",
      "        [2899,  756, 2359, 3845, 3771, 3862,    0,    0],\n",
      "        [ 733, 7599, 4788, 3857,  833, 3300, 3198,    0],\n",
      "        [7599, 4788, 3857,  833, 3300, 3198, 8024,    0],\n",
      "        [4788, 3857,  833, 3300, 3198, 8024, 4684,    0],\n",
      "        [3857,  833, 3300, 3198, 8024, 4684, 2899,    0],\n",
      "        [ 833, 3300, 3198, 8024, 4684, 2899,  756,    0],\n",
      "        [3300, 3198, 8024, 4684, 2899,  756, 2359,    0],\n",
      "        [3198, 8024, 4684, 2899,  756, 2359, 3845,    0],\n",
      "        [8024, 4684, 2899,  756, 2359, 3845, 3771,    0],\n",
      "        [4684, 2899,  756, 2359, 3845, 3771, 3862,    0],\n",
      "        [ 733, 7599, 4788, 3857,  833, 3300, 3198, 8024],\n",
      "        [7599, 4788, 3857,  833, 3300, 3198, 8024, 4684],\n",
      "        [4788, 3857,  833, 3300, 3198, 8024, 4684, 2899],\n",
      "        [3857,  833, 3300, 3198, 8024, 4684, 2899,  756],\n",
      "        [ 833, 3300, 3198, 8024, 4684, 2899,  756, 2359],\n",
      "        [3300, 3198, 8024, 4684, 2899,  756, 2359, 3845],\n",
      "        [3198, 8024, 4684, 2899,  756, 2359, 3845, 3771],\n",
      "        [8024, 4684, 2899,  756, 2359, 3845, 3771, 3862]])\n",
      "tensor([7599, 4788, 3857,  833, 3300, 3198, 8024, 4684, 2899,  756, 2359, 3845,\n",
      "        3771, 3862,  511, 4788, 3857,  833, 3300, 3198, 8024, 4684, 2899,  756,\n",
      "        2359, 3845, 3771, 3862,  511, 3857,  833, 3300, 3198, 8024, 4684, 2899,\n",
      "         756, 2359, 3845, 3771, 3862,  511,  833, 3300, 3198, 8024, 4684, 2899,\n",
      "         756, 2359, 3845, 3771, 3862,  511, 3300, 3198, 8024, 4684, 2899,  756,\n",
      "        2359, 3845, 3771, 3862,  511, 3198, 8024, 4684, 2899,  756, 2359, 3845,\n",
      "        3771, 3862,  511, 8024, 4684, 2899,  756, 2359, 3845, 3771, 3862,  511,\n",
      "        4684, 2899,  756, 2359, 3845, 3771, 3862,  511])\n",
      "输入序列: 乘 --> 标签: 风\n",
      "输入序列: 风 --> 标签: 破\n",
      "输入序列: 破 --> 标签: 浪\n",
      "输入序列: 浪 --> 标签: 会\n",
      "输入序列: 会 --> 标签: 有\n",
      "输入序列: 有 --> 标签: 时\n",
      "输入序列: 时 --> 标签: ，\n",
      "输入序列: ， --> 标签: 直\n",
      "输入序列: 直 --> 标签: 挂\n",
      "输入序列: 挂 --> 标签: 云\n",
      "输入序列: 云 --> 标签: 帆\n",
      "输入序列: 帆 --> 标签: 济\n",
      "输入序列: 济 --> 标签: 沧\n",
      "输入序列: 沧 --> 标签: 海\n",
      "输入序列: 海 --> 标签: 。\n",
      "输入序列: 乘风 --> 标签: 破\n",
      "输入序列: 风破 --> 标签: 浪\n",
      "输入序列: 破浪 --> 标签: 会\n",
      "输入序列: 浪会 --> 标签: 有\n",
      "输入序列: 会有 --> 标签: 时\n",
      "输入序列: 有时 --> 标签: ，\n",
      "输入序列: 时， --> 标签: 直\n",
      "输入序列: ，直 --> 标签: 挂\n",
      "输入序列: 直挂 --> 标签: 云\n",
      "输入序列: 挂云 --> 标签: 帆\n",
      "输入序列: 云帆 --> 标签: 济\n",
      "输入序列: 帆济 --> 标签: 沧\n",
      "输入序列: 济沧 --> 标签: 海\n",
      "输入序列: 沧海 --> 标签: 。\n",
      "输入序列: 乘风破 --> 标签: 浪\n",
      "输入序列: 风破浪 --> 标签: 会\n",
      "输入序列: 破浪会 --> 标签: 有\n",
      "输入序列: 浪会有 --> 标签: 时\n",
      "输入序列: 会有时 --> 标签: ，\n",
      "输入序列: 有时， --> 标签: 直\n",
      "输入序列: 时，直 --> 标签: 挂\n",
      "输入序列: ，直挂 --> 标签: 云\n",
      "输入序列: 直挂云 --> 标签: 帆\n",
      "输入序列: 挂云帆 --> 标签: 济\n",
      "输入序列: 云帆济 --> 标签: 沧\n",
      "输入序列: 帆济沧 --> 标签: 海\n",
      "输入序列: 济沧海 --> 标签: 。\n",
      "输入序列: 乘风破浪 --> 标签: 会\n",
      "输入序列: 风破浪会 --> 标签: 有\n",
      "输入序列: 破浪会有 --> 标签: 时\n",
      "输入序列: 浪会有时 --> 标签: ，\n",
      "输入序列: 会有时， --> 标签: 直\n",
      "输入序列: 有时，直 --> 标签: 挂\n",
      "输入序列: 时，直挂 --> 标签: 云\n",
      "输入序列: ，直挂云 --> 标签: 帆\n",
      "输入序列: 直挂云帆 --> 标签: 济\n",
      "输入序列: 挂云帆济 --> 标签: 沧\n",
      "输入序列: 云帆济沧 --> 标签: 海\n",
      "输入序列: 帆济沧海 --> 标签: 。\n",
      "输入序列: 乘风破浪会 --> 标签: 有\n",
      "输入序列: 风破浪会有 --> 标签: 时\n",
      "输入序列: 破浪会有时 --> 标签: ，\n",
      "输入序列: 浪会有时， --> 标签: 直\n",
      "输入序列: 会有时，直 --> 标签: 挂\n",
      "输入序列: 有时，直挂 --> 标签: 云\n",
      "输入序列: 时，直挂云 --> 标签: 帆\n",
      "输入序列: ，直挂云帆 --> 标签: 济\n",
      "输入序列: 直挂云帆济 --> 标签: 沧\n",
      "输入序列: 挂云帆济沧 --> 标签: 海\n",
      "输入序列: 云帆济沧海 --> 标签: 。\n",
      "输入序列: 乘风破浪会有 --> 标签: 时\n",
      "输入序列: 风破浪会有时 --> 标签: ，\n",
      "输入序列: 破浪会有时， --> 标签: 直\n",
      "输入序列: 浪会有时，直 --> 标签: 挂\n",
      "输入序列: 会有时，直挂 --> 标签: 云\n",
      "输入序列: 有时，直挂云 --> 标签: 帆\n",
      "输入序列: 时，直挂云帆 --> 标签: 济\n",
      "输入序列: ，直挂云帆济 --> 标签: 沧\n",
      "输入序列: 直挂云帆济沧 --> 标签: 海\n",
      "输入序列: 挂云帆济沧海 --> 标签: 。\n",
      "输入序列: 乘风破浪会有时 --> 标签: ，\n",
      "输入序列: 风破浪会有时， --> 标签: 直\n",
      "输入序列: 破浪会有时，直 --> 标签: 挂\n",
      "输入序列: 浪会有时，直挂 --> 标签: 云\n",
      "输入序列: 会有时，直挂云 --> 标签: 帆\n",
      "输入序列: 有时，直挂云帆 --> 标签: 济\n",
      "输入序列: 时，直挂云帆济 --> 标签: 沧\n",
      "输入序列: ，直挂云帆济沧 --> 标签: 海\n",
      "输入序列: 直挂云帆济沧海 --> 标签: 。\n",
      "输入序列: 乘风破浪会有时， --> 标签: 直\n",
      "输入序列: 风破浪会有时，直 --> 标签: 挂\n",
      "输入序列: 破浪会有时，直挂 --> 标签: 云\n",
      "输入序列: 浪会有时，直挂云 --> 标签: 帆\n",
      "输入序列: 会有时，直挂云帆 --> 标签: 济\n",
      "输入序列: 有时，直挂云帆济 --> 标签: 沧\n",
      "输入序列: 时，直挂云帆济沧 --> 标签: 海\n",
      "输入序列: ，直挂云帆济沧海 --> 标签: 。\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(\"./BERT中文词典\")\n",
    "\n",
    "# 输入文本\n",
    "text = \"乘风破浪会有时，直挂云帆济沧海。\"\n",
    "\n",
    "# 分词\n",
    "tokens = bert_tokenizer.tokenize(text)\n",
    "\n",
    "# 限制最大序列长度\n",
    "max_len = 8\n",
    "min_len = 1\n",
    "\n",
    "# 生成训练数据\n",
    "input_ids = []\n",
    "labels = []\n",
    "\n",
    "# 遍历不同的序列长度\n",
    "for seq_len in range(min_len, max_len + 1):\n",
    "    for i in range(len(tokens) - seq_len):\n",
    "        # 获取当前的输入序列\n",
    "        input_seq = tokens[i:i+seq_len]\n",
    "        # 获取下一个token作为标签\n",
    "        label_seq = tokens[i+seq_len] if i+seq_len < len(tokens) else None\n",
    "        \n",
    "        # 将token转化为id\n",
    "        input_ids_seq = bert_tokenizer.convert_tokens_to_ids(input_seq)\n",
    "        label_id = bert_tokenizer.convert_tokens_to_ids([label_seq])[0] if label_seq else -1  # 如果没有标签则使用-1作为填充值\n",
    "        \n",
    "        # 保存输入和标签\n",
    "        input_ids.append(input_ids_seq)\n",
    "        labels.append(label_id)\n",
    "        \n",
    "print(input_ids)\n",
    "print(labels)\n",
    "\n",
    "# 填充输入序列，使其长度一致\n",
    "# 使用 pad_token_id 填充\n",
    "input_ids_padded = pad_sequence([torch.tensor(seq) for seq in input_ids], \n",
    "                                batch_first=True, \n",
    "                                padding_value=bert_tokenizer.pad_token_id)\n",
    "\n",
    "# 转换为PyTorch tensor\n",
    "labels_tensor = torch.tensor(labels)\n",
    "\n",
    "# 打印tensor数据\n",
    "print(input_ids_padded.shape, labels_tensor.shape)\n",
    "print(input_ids_padded)\n",
    "print(labels_tensor)\n",
    "\n",
    "# 打印前几个样本\n",
    "for i in range(len(input_ids_padded)): \n",
    "    # 解码输入序列，过滤掉 pad_token_id\n",
    "    input_ids_filtered = [id for id in input_ids_padded[i].tolist() if id != bert_tokenizer.pad_token_id]\n",
    "    input_seq_decoded = bert_tokenizer.decode(input_ids_filtered, skip_special_tokens=True)\n",
    "    \n",
    "    # 解码标签序列，如果标签是-1则跳过\n",
    "    if labels_tensor[i].item() != -1:\n",
    "        label_decoded = bert_tokenizer.decode([labels_tensor[i].item()], skip_special_tokens=True)\n",
    "    else:\n",
    "        label_decoded = \"N/A\"  # 如果标签是无效的，标记为 N/A\n",
    "    \n",
    "    print(f\"输入序列: {input_seq_decoded.replace(' ','')} --> 标签: {label_decoded}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72b9c5e1-7fbc-497d-bb67-3c4dfe4d239b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing seq_len 1: 100%|███████████████████████████████████████████| 5271128/5271128 [00:22<00:00, 237381.88token/s]\n",
      "Processing seq_len 2: 100%|███████████████████████████████████████████| 5271127/5271127 [00:29<00:00, 181250.14token/s]\n",
      "Processing seq_len 3: 100%|███████████████████████████████████████████| 5271126/5271126 [00:36<00:00, 145816.78token/s]\n",
      "Processing seq_len 4: 100%|███████████████████████████████████████████| 5271125/5271125 [00:42<00:00, 123710.35token/s]\n",
      "Processing seq_len 5: 100%|███████████████████████████████████████████| 5271124/5271124 [00:50<00:00, 103940.46token/s]\n",
      "Processing seq_len 6: 100%|████████████████████████████████████████████| 5271123/5271123 [00:57<00:00, 91305.72token/s]\n",
      "Processing seq_len 7: 100%|████████████████████████████████████████████| 5271122/5271122 [01:05<00:00, 80879.33token/s]\n",
      "Processing seq_len 8: 100%|████████████████████████████████████████████| 5271121/5271121 [01:08<00:00, 76446.81token/s]\n",
      "Padding sequences: 100%|█████████████████████████████████████████| 42168996/42168996 [02:40<00:00, 263499.28sequence/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 12min 59s\n",
      "Wall time: 13min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from transformers import BertTokenizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 读取语料库\n",
    "with open('./data/people.cn/news.txt', 'r', encoding='utf-8') as f:\n",
    "    corpus = f.read()\n",
    "train_data = corpus[:1000000]\n",
    "val_data = corpus[-50000:]\n",
    "\n",
    "# 假设这里的 bert_tokenizer 已经初始化并可用\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('./BERT中文词典')\n",
    "\n",
    "# 整个语料库分词\n",
    "bert_tokens = bert_tokenizer.tokenize(corpus)\n",
    "\n",
    "# 限制最大序列长度\n",
    "max_len = 8\n",
    "min_len = 1\n",
    "\n",
    "# 生成训练数据\n",
    "input_ids = []\n",
    "labels = []\n",
    "\n",
    "# 遍历不同的序列长度\n",
    "for seq_len in range(min_len, max_len + 1):\n",
    "    for i in tqdm(range(len(bert_tokens) - seq_len), desc=f'Processing seq_len {seq_len}', unit='token'):\n",
    "        # 获取当前的输入序列\n",
    "        input_seq = bert_tokens[i:i + seq_len]\n",
    "        # 获取下一个token作为标签\n",
    "        label_seq = bert_tokens[i + seq_len] if i + seq_len < len(bert_tokens) else None\n",
    "        \n",
    "        # 将token转化为id\n",
    "        input_ids_seq = bert_tokenizer.convert_tokens_to_ids(input_seq)\n",
    "        # 如果没有标签则使用-1作为填充值\n",
    "        label_id = bert_tokenizer.convert_tokens_to_ids([label_seq])[0] if label_seq else -1\n",
    "        \n",
    "        # 保存输入和标签\n",
    "        input_ids.append(input_ids_seq)\n",
    "        labels.append(label_id)\n",
    "\n",
    "# 填充输入序列，使其长度一致，使用 pad_token_id 填充\n",
    "input_ids_padded = pad_sequence([torch.tensor(seq) for seq in \n",
    "                                tqdm(input_ids, desc=\"Padding sequences\", unit=\"sequence\")], \n",
    "                                batch_first=True, \n",
    "                                padding_value=bert_tokenizer.pad_token_id)\n",
    "\n",
    "# 转换为 PyTorch tensor\n",
    "labels_tensor = torch.tensor(labels)\n",
    "\n",
    "# 创建自定义 Dataset 类\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, input_ids, labels):\n",
    "        self.input_ids = input_ids\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.labels[idx]\n",
    "\n",
    "# 创建 TextDataset 实例\n",
    "dataset = TextDataset(input_ids_padded, labels_tensor)\n",
    "\n",
    "# 保存数据集到磁盘\n",
    "torch.save(dataset, 'data/text_dataset.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d34855a-efa4-4d75-aa36-e2172045812b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "批次 1:\n",
      "输入序列： tensor([[4385,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [ 852, 6632, 3221, 6821,    0,    0,    0,    0],\n",
      "        [2137, 6206, 5050, 1920, 6572,    0,    0,    0],\n",
      "        [4638, 5295, 1737, 2669,  821,  100,    0,    0]])\n",
      "标签： tensor([3291,  702,  510, 3124])\n",
      "\n",
      "批次 2:\n",
      "输入序列： tensor([[ 872,  812, 4638,    0,    0,    0,    0,    0],\n",
      "        [2398, 2190, 1912, 2458, 3123, 4638, 1104, 2552],\n",
      "        [5276, 8612,    0,    0,    0,    0,    0,    0],\n",
      "        [ 868, 4500,  511, 1126,    0,    0,    0,    0]])\n",
      "标签： tensor([2968, 1469, 1399, 1079])\n",
      "\n",
      "批次 3:\n",
      "输入序列： tensor([[5273, 5682, 3736,    0,    0,    0,    0,    0],\n",
      "        [4415, 6389,    0,    0,    0,    0,    0,    0],\n",
      "        [ 772, 6631, 8623,  674,    0,    0,    0,    0],\n",
      "        [2521, 2130, 1587, 8024,    0,    0,    0,    0]])\n",
      "标签： tensor([2255, 1158, 1039, 2213])\n",
      "\n",
      "批次 4:\n",
      "输入序列： tensor([[1277, 6816, 3341,  749,    0,    0,    0,    0],\n",
      "        [1301, 3813,    0,    0,    0,    0,    0,    0],\n",
      "        [4050,  691, 1285, 1762, 4522, 6241, 5089,  677],\n",
      "        [ 752, 8024, 2769, 6963, 1104,    0,    0,    0]])\n",
      "标签： tensor([ 671, 4872, 1091, 2552])\n",
      "\n",
      "批次 5:\n",
      "输入序列： tensor([[6381, 8024, 2273, 2272, 4007, 3184,    0,    0],\n",
      "        [1164, 6956, 2199, 6822,  671, 3635,    0,    0],\n",
      "        [2694, 1062, 4919,    0,    0,    0,    0,    0],\n",
      "        [3726,    0,    0,    0,    0,    0,    0,    0]])\n",
      "标签： tensor([5632, 2128, 2255, 1905])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "# 重新定义 TextDataset 类\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, input_ids, labels):\n",
    "        self.input_ids = input_ids\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.labels[idx]\n",
    "\n",
    "# 加载保存的 Dataset 数据集\n",
    "loaded_dataset = torch.load('data/text_dataset.pt', weights_only=False)\n",
    "\n",
    "# 使用 DataLoader 进行批量加载\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataloader = DataLoader(loaded_dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "# 测试加载的 DataLoader\n",
    "for batch_idx, (batchX, batchY) in enumerate(dataloader):\n",
    "    print(f\"批次 {batch_idx + 1}:\")\n",
    "    print(\"输入序列：\", batchX)\n",
    "    print(\"标签：\", batchY)\n",
    "    print()\n",
    "    if batch_idx + 1 >= 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91aae664-bc82-42c9-8d88-eccaad8778e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing seq_len 1: 100%|███████████████████████████████████████████████| 47767/47767 [00:00<00:00, 176564.03token/s]\n",
      "Processing seq_len 2: 100%|███████████████████████████████████████████████| 47766/47766 [00:00<00:00, 146713.12token/s]\n",
      "Processing seq_len 3: 100%|███████████████████████████████████████████████| 47765/47765 [00:00<00:00, 133926.60token/s]\n",
      "Processing seq_len 4: 100%|███████████████████████████████████████████████| 47764/47764 [00:00<00:00, 108760.92token/s]\n",
      "Processing seq_len 5: 100%|████████████████████████████████████████████████| 47763/47763 [00:00<00:00, 94799.03token/s]\n",
      "Processing seq_len 6: 100%|████████████████████████████████████████████████| 47762/47762 [00:00<00:00, 80296.69token/s]\n",
      "Processing seq_len 7: 100%|████████████████████████████████████████████████| 47761/47761 [00:00<00:00, 74775.97token/s]\n",
      "Processing seq_len 8: 100%|████████████████████████████████████████████████| 47760/47760 [00:00<00:00, 70775.08token/s]\n",
      "Padding sequences: 100%|█████████████████████████████████████████████| 382108/382108 [00:01<00:00, 250077.69sequence/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 13.5 s\n",
      "Wall time: 17 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from transformers import BertTokenizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 读取语料库\n",
    "with open('./data/people.cn/news.txt', 'r', encoding='utf-8') as f:\n",
    "    corpus = f.read()\n",
    "# train_data = corpus[:1000000]\n",
    "val_data = corpus[-50000:]\n",
    "\n",
    "# 假设这里的 bert_tokenizer 已经初始化并可用\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('./BERT中文词典')\n",
    "\n",
    "# 整个语料库分词\n",
    "bert_tokens = bert_tokenizer.tokenize(val_data)\n",
    "\n",
    "# 限制最大序列长度\n",
    "max_len = 8\n",
    "min_len = 1\n",
    "\n",
    "# 生成训练数据\n",
    "input_ids = []\n",
    "labels = []\n",
    "\n",
    "# 遍历不同的序列长度\n",
    "for seq_len in range(min_len, max_len + 1):\n",
    "    for i in tqdm(range(len(bert_tokens) - seq_len), desc=f'Processing seq_len {seq_len}', unit='token'):\n",
    "        # 获取当前的输入序列\n",
    "        input_seq = bert_tokens[i:i + seq_len]\n",
    "        # 获取下一个token作为标签\n",
    "        label_seq = bert_tokens[i + seq_len] if i + seq_len < len(bert_tokens) else None\n",
    "        \n",
    "        # 将token转化为id\n",
    "        input_ids_seq = bert_tokenizer.convert_tokens_to_ids(input_seq)\n",
    "        # 如果没有标签则使用-1作为填充值\n",
    "        label_id = bert_tokenizer.convert_tokens_to_ids([label_seq])[0] if label_seq else -1\n",
    "        \n",
    "        # 保存输入和标签\n",
    "        input_ids.append(input_ids_seq)\n",
    "        labels.append(label_id)\n",
    "\n",
    "# 填充输入序列，使其长度一致，使用 pad_token_id 填充\n",
    "input_ids_padded = pad_sequence([torch.tensor(seq) for seq in \n",
    "                                tqdm(input_ids, desc=\"Padding sequences\", unit=\"sequence\")], \n",
    "                                batch_first=True, \n",
    "                                padding_value=bert_tokenizer.pad_token_id)\n",
    "\n",
    "# 转换为 PyTorch tensor\n",
    "labels_tensor = torch.tensor(labels)\n",
    "\n",
    "# 创建自定义 Dataset 类\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, input_ids, labels):\n",
    "        self.input_ids = input_ids\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.labels[idx]\n",
    "\n",
    "# 创建 TextDataset 实例\n",
    "dataset = TextDataset(input_ids_padded, labels_tensor)\n",
    "\n",
    "# 保存数据集到磁盘\n",
    "torch.save(dataset, 'data/val_dataset.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dac9e56-e525-4172-a1c1-0ec3fab40d4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
