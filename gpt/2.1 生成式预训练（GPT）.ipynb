{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <b>2.1.1 预训练数据集</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20210101-01-01</td>\n",
       "      <td>20210101</td>\n",
       "      <td>中俄两国元首互致新年贺电 中俄两国总理互致新年贺电</td>\n",
       "      <td>新华社北京12月31日电 2020年12月31日...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20210101-01-02</td>\n",
       "      <td>20210101</td>\n",
       "      <td>国家主席习近平发表二〇二一年新年贺词</td>\n",
       "      <td>■ 2020年是极不平凡的一年。面对突如其来的新...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20210101-01-03</td>\n",
       "      <td>20210101</td>\n",
       "      <td>艰难方显勇毅，磨砺始得玉成 ——习近平主席二〇二一年...</td>\n",
       "      <td>新故相推，日生不滞。在风雨兼程中，我们告别202...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20210101-01-04</td>\n",
       "      <td>20210101</td>\n",
       "      <td>全国政协举行新年茶话会 习近平发表重要讲话李克强栗战...</td>\n",
       "      <td>新华社北京12月31日电 中国人民政治协商会议全...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20210101-02-01</td>\n",
       "      <td>20210101</td>\n",
       "      <td>在全国政协新年茶话会上的讲话 （2020年12月31日）</td>\n",
       "      <td>同志们，朋友们：\\n　　在风雨兼程中，我们即将送别2...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               id      date                          title  \\\n",
       "0  20210101-01-01  20210101     中俄两国元首互致新年贺电 中俄两国总理互致新年贺电    \n",
       "1  20210101-01-02  20210101            国家主席习近平发表二〇二一年新年贺词    \n",
       "2  20210101-01-03  20210101  艰难方显勇毅，磨砺始得玉成 ——习近平主席二〇二一年...   \n",
       "3  20210101-01-04  20210101  全国政协举行新年茶话会 习近平发表重要讲话李克强栗战...   \n",
       "4  20210101-02-01  20210101  在全国政协新年茶话会上的讲话 （2020年12月31日）    \n",
       "\n",
       "                         content  \n",
       "0  　　新华社北京12月31日电 2020年12月31日...  \n",
       "1  　　■ 2020年是极不平凡的一年。面对突如其来的新...  \n",
       "2  　　新故相推，日生不滞。在风雨兼程中，我们告别202...  \n",
       "3  　　新华社北京12月31日电 中国人民政治协商会议全...  \n",
       "4  同志们，朋友们：\\n　　在风雨兼程中，我们即将送别2...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# 设置最大列宽\n",
    "pd.set_option('display.max_colwidth',30) \n",
    "news = pd.read_csv('./data/people.cn/RenMin_Daily.csv')\n",
    "news.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   id      date                          title  \\\n",
      "30000  20220412-12-02  20220412  重庆白鹤梁水下博物馆用科技手段保障文物安全 千年题刻...   \n",
      "30001  20220412-12-03  20220412                  甘肃出台方案加强科技创新    \n",
      "30002  20220412-12-04  20220412       挖掘永泰庄寨的价值（新语·让好声音成为最强音）    \n",
      "\n",
      "                             content  \n",
      "30000  　　乘坐长91米的隧道式自动扶梯渐入水下，横穿146...  \n",
      "30001  　　本报兰州4月11日电  （记者付文）11日，甘肃...  \n",
      "30002  　　承载着村落历史、维系着村民记忆的乡村文化遗产，能...  \n"
     ]
    }
   ],
   "source": [
    "print(news.iloc[30000:30003])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "行数: 90719, 列数: 4\n"
     ]
    }
   ],
   "source": [
    "# 查看行数和列数\n",
    "rows, cols = news.shape\n",
    "print(f'行数: {rows}, 列数: {cols}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将数据表中content列所有的资讯提取，合并到一个文本文件中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文本已成功写入 news.txt 文件！\n"
     ]
    }
   ],
   "source": [
    "# 将目标列转换为字符串,因为资讯中可能含有浮点数字等\n",
    "news['content'] = news['content'].astype(str)\n",
    "\n",
    "# 个人电脑处理400MB的文本还是太大了，所以指定行号列表,抽取部分数据做实验，例如1/25的数据量\n",
    "rows = [x for x in range(1, news.shape[0]) if x % 25 == 0]  # 筛选数据行\n",
    "\n",
    "# 提取目标列（列名为 'content'）\n",
    "texts = news['content'].iloc[rows]\n",
    "\n",
    "# 分段合并（每段以两个换行分隔）\n",
    "merged_text = '\\n\\n'.join(texts)\n",
    "\n",
    "# 将结果写入文本文件\n",
    "with open('./data/people.cn/news.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(merged_text)\n",
    "\n",
    "print(\"文本已成功写入 news.txt 文件！\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "818"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "# 使用完 数据集 后删除\n",
    "del news, texts, merged_text\n",
    "# 调用垃圾回收\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "O6medjfRsLD9"
   },
   "outputs": [],
   "source": [
    "# 读取新数据集\n",
    "with open('./data/people.cn/news.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6xWI_VyAsN8F",
    "outputId": "ed819dd0-72e5-40a6-d2ed-928ff73bfda6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据集包含的字符数量: 5,519,725 个\n"
     ]
    }
   ],
   "source": [
    "print(f'数据集包含的字符数量: {len(text):,} 个')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "　　新华社北京12月31日电  国务院办公厅日前印发《关于促进养老托育服务健康发展的意见》（以下简称《意见》）。\n",
      "　　《意见》指出，促进养老托育服务健康发展，有利于改善民生福祉，有利于促进家庭和谐，有利于培育经济发展新动能。为贯彻落实党中央、国务院决策部署，更好发挥各级政府作用，更充分激发社会力量活力，更好实现社会效益和经济效益相统一，持续提高人民群众的获得感、幸福感、安全感，《意见》就促进养老托\n"
     ]
    }
   ],
   "source": [
    "# 开头150个字符\n",
    "print(text[ :200]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "2c5V0FvqseE0",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "25ca7adc-b8c0-42d1-b08c-e0863c5c314e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "个，乡镇影院银幕超过1.2万块，近20万家农家书屋提供数字阅读服务。所有公共图书馆、文化馆、美术馆、综合文化站和超91%的博物馆免费开放。广大人民群众的生活方式与精神风貌为之一新。\n",
      "　　从文旅融合中激发消费潜力\n",
      "　　茵茵绿草旁，悠游漫步，欣赏文化市集的创意；潺潺流水边，露营野餐，享受“公园20分钟”的惬意。在湖北当阳花溪湿地，一系列文旅活动与自然风光相映成趣，市民游客尽享假期的“文化味儿”。\n",
      "　　“和家人一起来这里露营，放松身心。今年新增了三楚书屋，花溪湿地更多了些诗意。”市民李睿说。\n",
      "　　“诗和远方”的向往，使文旅消费持续升温。国庆假期，人们外出旅游热情高涨，多地迎来客流高峰，文旅市场呈现一片红火景象。10月1日，全社会跨区域人员流动量超3.3亿人次，同比实现增长。\n",
      "　　“把自然风光和人文风情转化为旅游业的持久魅力”，关键在以文塑旅、以旅彰文，挖掘旅游中的文化竞争力，让山水人文从“可观”到“可游”、让市民游客从“看景”到“入景”。\n",
      "　　国庆假期，重庆江北北仓文创街区人流涌动。“这里的老重庆风貌有特色，文化体验活动有意思，我们年轻人喜欢来。”10月2日上午，在陶画园文学手工馆体验手工陶艺的游客冯露说。\n",
      "　　北仓文创街区由塔坪片区内的老纺织仓库改造而来，集城市图书馆、生活美学馆、创客空间于一体，已入驻80户商户，年综合营收超过1亿元，直接带动就业超过600人。\n",
      "　　文旅融合带动“一业兴、百业旺”。各地顺应消费新趋势，积极打造文化新业态、新场景，持续提升消费结构，满足不同消费者需求。\n",
      "　　在乡村，体验活力四射的“村晚”“村超”“村BA”。广场旁的集市，农产品和非遗产品热销，将乡村文化活动转化为文旅消费活动。1597个全国乡村旅游重点村镇示范引领，文旅消费热度不断上升。\n",
      "　　在城市，打卡创意十足的文化街区、夜间集市。345个国家级夜间文化和旅游消费集聚区搭建消费场景，42家智慧旅游沉浸式体验新空间培育试点项目开发新型服务，文旅消费动能持续释放。\n",
      "　　“一个国家、一个民族的强盛，总是以文化兴盛为支撑的”。\n",
      "　　今天的中国，文脉传承弦歌不辍，文化产业蓬勃发展，文化事业生机盎然，人民享有更加充实、更为丰富、更高质量的精神文化生活，必将凝聚起强国建设、民族复兴的强大精神力量。\n",
      "　　（本报记者张丹华、曹玲娟、陈隽逸、姚雪青、宋朝军、张文豪、郑海鸥、韩俊杰、庞革平、刘新吾参与采写）\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 末尾1000个字符\n",
    "print(text[-1000:]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <b>2.1.2 分词与词典</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "针对语料库，自行定义词典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "0e-Rbyr8sfM8",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "f34e94a9-5b44-4cf3-885b-986731929109"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !#%&'()*+,-./0123456789:=>@ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz °±·×èπ—‘’“”…‰℃™ⅠⅡⅢⅣⅤ∶①②③④⑤⑥⑦⑧⑨⑩■▶◆●　、。〇〈〉《》【】〔〕㎡一丁七万丈三上下不与丑专且丕世丘丙业丛东丝丢两严丧丨个丫中丰串临丸丹为主丽举乃久么义之乌乍乎乏乐乒乓乔乘乙九乞也习乡书买乱乳乾了予争事二于亏云互亓五井亘亚些亟亡亢交亥亦产亨亩享京亭亮亲亳亵亶人亿什仁仃仄仅仆仇仉今介仍从仑仓仔仕他仗付仙仚仝仞仡代令以仪仫们仰仲仵件价任份仿企伊伍伎伏伐休众优伙会伞伟传伤伥伦伪伫伯估伴伶伸伺似伽佃但位低住佐佑体何佘余佚佛作佟你佣佤佩佬佯佰佳佶佺佼使侃侄侈例侍侏侑侗供依侠侣侥侦侧侨侬侮侯侵便促俄俊俎俏俐俑俗俘俚保俞俟信俤俨俩俭修俯俱俺俾倍倏倒倔倘候倚倜借倡倦倩倪倭债值倾偃假偌偎偏偕做停健偲偶偷偾偿傅傈傍傣傥傧储傩催傲傻像僚僧僮僳僵僻儆儋儒儿兀允元兄充兆先光克免兑兔党兜兢入全八公六兮兰共关兴兵其具典兹养兼兽冀内冈冉册再冒冕冗写军农冠冢冤冥冬冯冰冲决况冶冷冻冼冽净凄准凇凉凋凌减凑凛凝几凡凤凭凯凰凳凶凸凹出击函凿刀刁刃分切刊刍刑划列刘则刚创初删判刨利别刮到制刷券刹刺刻剁剂剅削前剐剑剔剖剥剧剩剪副割剿劈力劝办功加务劣动助努劫励劲劳劵劼势勃勇勉勋勐勒勘募勠勤勺勾勿匀包匆匈匍匐化北匙匝匠匡匣匪匮匹区医匾匿十千卅升午卉半华协卑卒卓单卖南博卜卞占卡卢卤卦卧卨卫卯印危即却卵卷卸卿厂厄厅历厉压厌厕厘厚厝原厢厥厦厨厮去县参又叉及友双反发叔取受变叙叛叠口古句另叨叩只叫召叭叮可台叱史右叵叶号司叹叼叽吁吃各吆合吉吊同名后吏吐向吒吓吕吗君吝吞吟否吧吨含听吭吮启吱吴吵吸吹吻吼吾呀呆呈告呐呕呗员呙呛呜呢呤呦周呲味呵呷呼命咀咂咄咋和咎咏咐咔咕咖咙咚咝咤咦咧咨咫咬咯咱咳咸咽咿哀品哄哆哇哈哉响哎哑哒哔哗哟哥哦哧哨哩哪哭哮哲哺哼哽唁唆唇唉唏唐唠唢唤唧唬售唯唰唱啁啃啄商啊啜啡啤啥啦啧啪啬啭啰啵啶啸啻啼啾喀喂喃善喆喇喉喊喏喘喙喜喝喧喳喷喹喻喽喾嗅嗑嗒嗓嗔嗜嗡嗣嗤嗦嗬嗽嘀嘈嘉嘌嘎嘘嘛嘞嘟嘣嘤嘭嘱嘴嘶嘹嘻嘿噌噎噔噗噙噜噢噤器噩噪噬噱噶噼嚎嚏嚓嚣嚯嚷嚼囊囚四回因团囤囫园困囱围囵囹固国图囿圃圄圆圈圐圙圜土圣在圩圪圭地圳场圾址坂均坊坌坍坎坏坐坑块坚坛坜坝坞坟坠坡坤坦坨坩坪坬坭坯坳坷坻垂垃垄型垌垒垚垛垠垢垣垦垧垩垫垭垮埂埃埇埋城埔埕埚埝域埠埭培基堀堂堃堆堇堉堑堕堞堡堤堪堰堵堽塄塆塌塑塔塘塞填塬塾墀境墅墈墒墓墙增墟墨墩壁壑壕壤士壬壮声壳壶壹处备复夏夔夕外夙多夜够大天太夫夭央夯失头夷夸夹夺夼奁奂奄奇奈奉奋奎奏契奔奕奖套奘奚奠奢奥奭女奴奶奸她好如妃妄妆妇妈妊妍妖妙妤妥妨妩妮妯妹妻姆姊始姐姑姓委姗姚姜姝姣姥姨姬姹姻姿威娃娄娅娇娉娌娑娓娘娜娟娠娣娥娩娱娲娴娶婀婆婉婕婚婧婪婴婵婶婷婺婿媒媚媛媲媳嫁嫂嫉嫌嫡嫣嫦嫩嫫嫱嬉嬗孀孃子孑孔孕孖字存孙孚孜孝孟孢季孤学孩孪孱孳孵孺宁它宅宇守安宋完宏宕宗官宙定宛宜宝实宠审客宣室宦宪宫宰害宴宵家宸容宽宾宿寀寂寄寅密寇富寐寒寓寝寞察寡寥寨寮寯寰寸对寺寻导寿封射将尉尊小少尔尕尖尘尚尝尤尧尬就尴尸尹尺尼尽尾尿局屁层居屈屉届屋屎屏屑展属屠屡履屯山屹屿岁岂岐岑岔岖岗岚岛岜岢岩岫岭岱岳岷岸岿峁峄峋峒峙峡峤峥峦峨峪峭峰峻崂崆崇崎崑崔崖崛崟崧崩崭崮崴崽嵇嵊嵋嵌嵘嵚嵛嵩嶂嶙嶝巅巉巍川州巡巢工左巧巨巩巫差己已巴巷巾币市布帅帆师希帐帕帖帘帙帚帛帜帝带帧席帮帷常帼帽幄幅幌幕幛幡幢干平年并幸幺幻幼幽广庄庆庇床序庐库应底庖店庙庚府庞废庠度座庭庵康庸庹庾廉廊廓廖廪延廷建廿开异弃弄弈弊弋式弓引弗弘弛弟张弥弦弧弩弭弯弱弶弹强弼归当录彝形彤彦彩彪彬彭彰影彷役彻彼往征径待徇很徉徊律徐徒徕得徘徙徜御徨循微徵德徽心必忆忌忍忐忑志忘忙忠忡忧快忱念忻忽忾怀态怂怅怆怎怒怕怖怜思怠怡急怦性怨怪怯总怿恃恋恍恐恒恕恙恢恣恤恨恩恪恬恭息恰恳恶恹恺恼恽恿悄悉悍悔悖悟悠患悦您悬悲悸悼情惆惊惋惑惕惘惚惜惟惠惦惧惨惩惫惬惭惮惯惰想惴惹惺愁愈愉意愚感愣愤愧愫愿慈慌慎慑慕慢慧慨慰慷憋憧憨憩憬憾懂懈懊懒懵懿戈戊戌戍戎戏成我戒或戗战戚戛戟截戮戳戴户戾房所扁扇扈扉手才扎扑扒打扔托扛扣扦执扩扫扬扭扮扯扰扳扶批扼找承技抄抉把抑抒抓抔投抖抗折抚抛抠抡抢护报抨披抬抱抵抹押抽抿拂担拆拇拈拉拌拍拎拐拒拓拔拖拗拘拙招拜拟拢拣拥拦拧拨择括拭拮拯拱拳拴拷拼拽拾拿持挂指挈按挎挑挖挚挛挝挟挠挡挣挤挥挨挪挫振挺挽捂捆捉捋捌捍捎捏捐捕捞损捡换捣捧据捶捷捺捻掀掂掇授掉掌掏掐排掖掘掠探掣接控推掩措掬掰掷掸掺揉描提插揠握揣揪揭援揺揽搀搁搂搅搏搐搓搜搞搪搬搭携摁摄摆摇摊摒摔摘摞摧摩摸摹撂撇撑撒撕撞撤撩撬播撮撰撵撸撼擀擂擅操擎擒擘擞擤擦攀攒攘攥攫支收攸改攻放政故效敌敏救敕敖教敛敝敞敢散敦敬数敲整敷文斋斌斐斑斓斗料斛斜斟斡斤斥斧斩斫断斯新方於施旁旅旋旌旎族旖旗无既日旦旧旨早旬旭旱时旷旸旺旻昀昂昆昇昉昊昌明昏易昔昕昙昝星映春昧昨昭是昱昵昶昼显晁晃晋晌晏晒晓晔晕晖晗晚晟晤晦晨晩普景晰晴晶晷智晾暂暄暇暑暖暗暧暨暮暴暹曌曙曜曝曦曰曲曳更曹曼曾替最月有朋服朔朗望朝期朦木未末本札术朱朴朵机朽杀杂权杆杈杉李杏材村杓杖杜杞束杠条来杨杭杯杰杲杳杵杷杼松板极构枇枉析枕林枚果枝枞枢枣枧枨枪枫枭枯架枷枸柄柏某柑柒染柔柘柚柜柞柠柢查柩柬柯柰柱柳柴柽柿栀栅标栈栉栋栌栎栏树栓栖栗校栢栩株栱样核根格栽栾桁桂桃桅框案桉桌桎桐桑桓桔桕桢档桥桦桨桩桫桶桷梁梅梆梏梓梗梢梦梧梨梭梯械梳梵检棂棉棋棍棒棕棘棚棠棣棨森棱棵棺椅植椎椒椠椤椭椰椴椽椿楂楚楞楠楣楫楷楸楹楼榀概榄榆榈榔榕榛榜榧榨榫榭榴榻槃槌槎槐槔槛槟槠槭槽槿樊樑樟模横樯樱樵樸樽橄橇橘橙橡橱橹檀檐檬欠次欢欣欧欲欺款歆歇歉歌歙止正此步武歧歪歹死歼殃殆殇殉殊残殓殖殚殡殴段殷殿毁毂毅毋母每毒毓比毕毖毗毙毛毡毫毯毳毽氏民气氚氛氟氢氤氦氧氨氩氮氯氰氲水永汀汁求汇汉汊汐汔汕汗汛汝汞江池污汤汨汩汪汰汲汴汶汹汽汾沁沂沃沅沆沈沉沏沐沓沔沙沚沛沟没沣沥沦沧沪沫沭沮沱河沸油治沼沽沾沿泃泄泉泊泌泓法泗泛泞泡波泣泥注泪泫泮泯泰泱泳泵泸泺泻泼泽泾洁洄洇洋洎洑洒洗洛洞津洨洪洮洱洲洵活洼洽派流浃浅浆浇浊测济浏浐浑浒浓浔浙浚浜浠浣浦浩浪浮浴海浸涂涅消涉涌涎涓涕涛涝涞涟涠涡涣涤润涧涨涩涪涮涯液涵涸涿淀淄淅淆淇淋淌淑淖淘淙淛淜淝淞淡淤淦淫淬淮深淳混淹添淼清渊渌渍渎渐渔渕渗渚渝渠渡渣渤渥温渭港渲渴游渺渿湃湄湉湍湎湖湘湛湟湮湾湿溃溅溆溉源溜溟溢溥溧溪溯溶溺溽滁滂滇滋滑滓滔滕滚滞滟满滢滤滥滦滨滩滴滹漂漆漉漏漓演漕漠漩漪漫漯漱漳漾潇潋潍潘潜潞潟潢潦潭潮潸潺潼潽澄澈澍澎澜澡澧澳澹激濂濉濒濛濞濠濡濮濯瀑瀚瀛瀣瀼灌灏灞火灭灯灰灵灶灸灼灾灿炉炊炎炒炕炖炙炜炫炬炭炮炯炳炸点炼炽烁烂烃烈烊烘烙烛烟烤烦烧烨烫烬热烮烯烷烹烺烽焉焊焐焕焖焘焙焚焜焦焯焰焱然煅煌煎煜煞煤煦照煨煮煲煽熄熊熏熔熙熟熠熨熬熳熹熺燃燎燕燚燥燧燮爆爪爬爱爵父爷爸爹爽牁牂片版牌牍牒牙牛牟牡牢牤牦牧物牲牵特牺犀犁犇犊犍犟犬犯状犷犸犹狂狄狍狐狗狠狡狩独狭狮狱狸狼猎猕猖猗猛猜猩猪猫猬献猯猴猾猿獐獗獠獭玄率玉王玎玑玖玛玟玠玥玩玫玮环现玲玳玷玺玻珀珂珉珊珍珏珐珑珙珞珠珥珩班珮珲珺球琅理琉琍琎琏琐琛琢琥琦琨琪琮琰琳琴琵琶琼瑀瑁瑄瑒瑕瑙瑚瑛瑜瑞瑟瑨瑰瑶瑷瑾璀璁璃璇璋璐璜璞璠璧璨璹瓜瓢瓣瓦瓮瓯瓴瓶瓷甄甘甚甜生用甩甫甬田由甲申电男甸町画畅畈界畏畔留畚畜略番畲畴畸畹畿疃疆疏疑疗疙疚疝疟疤疫疮疯疱疲疴疵疹疼疽疾病症痉痊痍痒痕痛痧痪痰痴痹痼瘁瘟瘠瘢瘤瘦瘩瘪瘫瘴瘸瘾癌癖癣癯癸登白百皂的皆皇皋皎皑皓皖皮皱皲皴皿盂盅盆盈盉益盎盏盐监盒盔盖盗盘盛盟盥目盯盱盲直相盹盼盾省眉看眙真眠眨眩眭眯眶眷眸眺眼着睁睇睐睛睡睢督睦睫睹睿瞄瞅瞒瞥瞧瞩瞪瞬瞭瞰瞳瞻瞿矍矗矛矜矢矣知矩矫短矮石矶矸矾矿砀码砂砌砍研砖砚砟砥砦砧砬砭砰破砷砸砺砻砼砾础硃硅硐硒硕硖硗硚硝硫硬确硷硼碇碉碌碍碎碑碓碗碘碚碛碟碡碣碧碰碱碳碶碾磁磅磊磋磐磕磘磜磨磬磴磷礁礴示礼礽社祀祁祈祉祎祖祛祜祝神祟祠祥票祭祯祷祸祺禀禁禄禅福禔禘禧禬禳禹禺离禽禾秀私秃秆秉秋种科秒秘租秣秤秦秧秩秭积称秸移秽稀程稍税稔稚稞稠稳稷稻稼稽稿穆穑穗穴究穷穹空穿突窃窄窅窋窍窑窒窖窗窘窜窝窟窠窣窥窦窨窸窿立竑竖站竞竟章竣童竭端竹竺竿笃笆笈笋笏笑笔笙笛笠符笨第笺笼筅等筋筏筐筑筒答策筚筛筜筝筠筱筷筹筻筼签简箍箐箔箕算管箩箫箭箱箴箸篁篆篇篑篓篝篡篪篮篱篷篾簇簋簧簪簸簿籁籍米类籼籽粉粑粒粕粗粘粝粟粤粥粪粮粱粲粳粹粼粽精糁糅糊糌糍糕糖糙糟糠糯系紊素索紧紫累絮綦綮縻繁纂纠红纣纤约级纪纫纬纭纯纱纲纳纵纶纷纸纹纺纽纾线练组绅细织终绊绍绎经绑绒结绕绘给绚绛络绝绞统绢绣绥绦继绩绪续绮绯绰绱绳维绵绶绷绸绺综绽绿缀缂缄缅缆缇缈缉缎缓缔缕编缘缙缚缛缜缝缟缠缤缥缨缩缪缭缮缰缴缸缺罄罐网罔罕罗罘罚罢罩罪置署罹羁羊羌美羔羚羞羡群羲羹羽羿翀翁翅翊翌翎翔翕翘翙翟翠翡翩翰翱翻翼耀老考耄者耆耋而耍耐耒耕耗耘耙耦耳耶耸耻耽耿聂聃聆聊聋职聒联聘聚聩聪聿肃肆肇肉肋肌肖肘肚肝肠股肢肤肥肩肪肯育肴肺肽肾肿胀胁胃胆背胍胎胖胚胜胝胞胡胥胧胫胭胯胰胳胶胸胺胼能脂脆脉脊脍脏脐脑脖脚脯脱脸脾腆腈腊腋腌腐腑腔腕腥腩腮腰腱腴腹腺腻腼腾腿膀膊膏膛膜膝膦膨膳膺臂臆臊臣臧自臬臭至致臻臼臾舀舅舆舌舍舒舜舞舟航舫般舰舱舳舵舶舷舸船舻艇艘良艰色艳艺艾节芃芊芋芒芗芙芜芝芥芦芩芫芬芭芮芯花芳芷芸芹芽芾苇苍苎苏苑苒苓苔苕苗苛苜苞苟苣若苦苯英苴苷苹苻茁茂范茄茅茆茉茎茏茗茚茜茧茨茫茬茭茯茵茶茸茹茼荀荃荆草荏荐荒荔荚荞荟荠荡荣荤荥荦荧荪荫药荷荸荻荼荽莅莆莉莎莒莓莘莞莠莫莱莲获莹莺莽菁菅菇菊菌菏菖菜菠菡菩菱菲菽萁萃萄萋萌萍萎萘萝营萦萧萨萱萼落葆著葚葛葡董葩葫葬葭葱葳葵葸葺蒂蒋蒗蒙蒜蒯蒲蒸蒿蓄蓉蓊蓑蓓蓝蓟蓥蓦蓬蓼蓿蔑蔓蔗蔚蔟蔡蔬蔷蔸蔺蔼蔽蕃蕉蕊蕙蕤蕨蕲蕴蕻蕾薄薅薇薛薜薤薪薮薯薰薹藁藉藏藓藕藜藤藩藻藿蘑蘖蘸虆虎虏虐虑虔虚虞虫虬虱虹虺虽虾蚀蚁蚂蚊蚌蚓蚕蚜蚝蚣蚤蚪蚯蚱蛀蛇蛉蛊蛋蛙蛛蛟蛤蛮蛰蛹蛾蜀蜂蜈蜒蜓蜕蜗蜘蜚蜜蜡蜷蜻蜿蝇蝉蝌蝎蝗蝴蝶螃螈融螟螭螯螺蟠蟥蟹蟾蠢蠹血衅行衍衔街衙衡衢衣补表衫衬衰衷袁袂袄袅袋袍袒袖袜袤被袭袱裁裂装裆裔裕裘裙裤裨裱裳裴裸裹褂褐褒褓褚褥褪褶襁襄襟西要覃覆见观规觅视览觉觊觎觑角觞解触言訚訾詹誉誊誓謇警譬计订认讥讨让讪训议讯记讲讳讴讶讷许讹论讼讽设访诀证诂诃评识诈诉诊诋词诏译试诗诘诙诚话诞诟诠诡询诣该详诧诩诫诬语误诱诲说诵请诸诺读诽课诿谀谁调谅谆谈谊谋谌谍谎谏谐谒谓谕谙谚谛谜谟谡谢谣谤谦谧谨谩谬谭谯谱谲谴谷豁豆豇豌豚象豪豫豸豹豺貂貌貘贝贞负贠贡财责贤败账货质贩贪贫贬购贮贯贰贱贴贵贷贸费贺贻贼贽贾贿赁赂赃资赅赈赉赊赋赌赏赐赓赔赖赘赚赛赞赟赠赡赢赣赤赦赫赭走赳赴赵赶起趁趄超越趋趔趟趣足趴趵趸跃跆跋跌跎跑跖跚跛距跟跤跨跪跬路跳践跷跹跻踊踌踏踔踝踞踢踩踪踮踱踵蹂蹄蹇蹈蹉蹊蹑蹒蹙蹚蹦蹬蹭蹲蹴蹿躁躇躏身躬躯躲躺車輖车轧轨轩轫转轮软轰轲轳轴轶轻轼载轾轿较辄辅辆辇辈辉辊辋辍辏辐辑输辕辖辗辘辙辛辜辞辟辣辨辩辫辰辱边辽达迁迂迄迅过迈迎运近返还这进远违连迟迢迤迥迦迩迪迫迭述迳迷迸迹追退送适逃逄逅逆选逊逍透逐递途逗通逛逝逞速造逢逦逮逯逶逸逻逼逾遁遂遇遍遏遐遑遒道遗遛遣遥遨遭遮遴遵避邀邂邃邑邓邕邗邝邢那邦邨邪邬邮邯邰邱邳邴邵邸邹邺邻郁郄郅郇郊郎郑郓郜郝郡郢郧部郫郭郯郴郸都鄂鄄鄞鄠鄯鄱酉酋酌配酒酗酚酝酣酥酩酬酮酯酰酱酵酶酷酸酽酿醇醉醋醒醛醴醺采釉释里重野量金釜鉴銮鋆鋬錾鏖鑫钇针钉钊钎钒钓钗钙钛钝钞钟钠钡钢钣钤钥钦钧钨钩钬钮钰钱钳钴钵钹钺钻钼钽钾铀铁铂铃铄铅铆铉铎铐铑铖铙铛铜铝铟铠铡铢铣铤铧铨铬铭铮铰铱铲铵银铸铺链铿销锁锂锃锄锅锈锋锌锏锐锗错锚锟锡锢锣锤锥锦锭键锯锰锲锴锵锸锹锻镀镁镂镇镉镊镌镍镐镑镓镔镕镖镗镛镜镝镠镣镩镭镰镶长门闪闫闭问闯闰闲闳间闵闷闸闹闺闻闽闾阀阁阂阅阆阈阎阐阑阔阖阗阙阚阜队阡阪阮阱防阳阴阵阶阻阿陀陂附际陆陇陈陉陋陌降限陕陟陡院除陨险陪陲陵陶陷隅隆隋隍随隐隔隘隙障隧隰隶隼隽难雀雁雄雅集雇雉雌雍雏雒雕雨雩雪雯雳零雷雹雾需霁霄霆震霉霍霎霏霓霖霜霞霭露霸霹霾青靓靖静靛非靠靡面靥革靳靴靶鞅鞋鞍鞠鞭韦韧韩韫韬韭音韵韶頴页顶顷项顺须顼顽顾顿颁颂预颅领颇颈颉颊颌颍颐频颓颔颖颗题颜额颠颤颦风飏飒飓飕飘飙飚飞食飨餐饥饨饪饬饭饮饰饱饲饴饵饶饸饺饼饽饿馁馃馄馅馆馈馋馍馏馒馓馕首香馥馨驩马驭驮驯驰驱驳驴驶驷驸驹驻驼驾驿骀骁骂骄骅骆骇骈骋验骏骑骗骚骛骞骡骢骤骥骨骸骼髋髓高髦髫髯髻鬃鬓鬻鬼鬽魁魂魄魅魏魔鮰鱀鱼鱿鲁鲃鲈鲍鲑鲚鲜鲟鲢鲤鲫鲮鲲鲶鲷鲸鳄鳅鳌鳍鳗鳙鳞鳟鵾鸟鸠鸡鸢鸣鸥鸦鸩鸫鸬鸭鸯鸳鸶鸽鸿鹂鹃鹄鹅鹈鹉鹊鹍鹏鹕鹚鹤鹦鹭鹮鹰鹳鹿麋麒麓麝麟麦麯麻麾黄黉黍黎黏黑黔默黛黝黟黢黩黯鼎鼐鼓鼠鼢鼯鼻鼾齐齿龃龄龉龘龙龚龛龟！％（）＋，：；＜＝＞？＠\n",
      "\n",
      "字典长度：5173\n"
     ]
    }
   ],
   "source": [
    "# 使用 set 提取唯一字符, 统计数据集（语料库）中不同字符\n",
    "unique_chars = set(text) \n",
    "\n",
    "# 按照字符的 Unicode 编码顺序排序，得到词典 vocab\n",
    "vocab = sorted(list(set(text))) \n",
    "\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "print(''.join(vocab))\n",
    "print(f'\\n字典长度：{vocab_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "词典大小: 5173\n",
      "词典保存为文本格式: data/news_vocab.txt\n",
      "词典大小: 5173\n",
      "词典保存为JSON格式： data/news_vocab.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# 根据文本串 text， 生成词典文件output_file，支持txt和json两种存储格式\n",
    "def generate_vocab(text, output_file, format='txt'):\n",
    "    \n",
    "    # 提取文本中的唯一字符并排序\n",
    "    unique_chars = set(text)\n",
    "    vocab = sorted(list(unique_chars))\n",
    "\n",
    "    # 计算词典大小\n",
    "    vocab_size = len(vocab)\n",
    "    print(f\"词典大小: {vocab_size}\")\n",
    "\n",
    "    # 创建词典，将每个字符映射到一个唯一的ID\n",
    "    vocab_dict = {char: idx for idx, char in enumerate(vocab)}\n",
    "\n",
    "    # 保存词典\n",
    "    if format == 'txt':\n",
    "        # 保存为文本格式\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            for char, idx in vocab_dict.items():\n",
    "                f.write(f\"{char}\\n\")\n",
    "        print(f\"词典保存为文本格式: {output_file}\")\n",
    "    \n",
    "    elif format == 'json':\n",
    "        # 保存为 JSON 格式\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(vocab_dict, f, ensure_ascii = False, indent = 4)\n",
    "        print(f\"词典保存为JSON格式： {output_file}\")\n",
    "    else:\n",
    "        print(\"不支持的文件格式，请使用 'txt'或 'json'格式\")\n",
    "\n",
    "# 示例：生成词典并保存为文件\n",
    "# corpus = \"这是一个自定义的语料库文本。文本中包含不同的字符。\"\n",
    "# generate_vocab(corpus, \"vocab1.txt\", format='txt')   # 保存为txt格式\n",
    "# generate_vocab(corpus, \"vocab1.json\", format='json') # 保存为json格式\n",
    "\n",
    "# 根据人民日报语料生成词典\n",
    "generate_vocab(text, \"data/news_vocab.txt\", format='txt')   # 保存为txt格式\n",
    "generate_vocab(text, \"data/news_vocab.json\", format='json') # 保存为json格式\n",
    "del text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "==================================================================================\n",
    "实现一个自定义的分词程序，自定义词典的加载与管理，具备类似 BERT 和 GPT 的分词功能：\n",
    "\n",
    "（1）**自定义分词Tokenizer**：实现一个分词器（Tokenizer），<br/>\n",
    "    提供 `convert_tokens_to_ids` 和 `convert_ids_to_tokens` 等方法。<br/>\n",
    "（2）**分词功能**：支持基于自定义词典的分词方法，类似于最大匹配（LPM）算法。<br/>\n",
    "\n",
    "##### `Tokenizer` 类\n",
    "\n",
    "这个类将包含：\n",
    "- `from_pretrained` 方法：从一个文件加载自定义词典。\n",
    "- `tokenize` 方法：对输入文本进行分词。\n",
    "- `convert_tokens_to_ids` 方法：将词汇转换为 ID。\n",
    "- `convert_ids_to_tokens` 方法：将 ID 转换回词汇。\n",
    "\n",
    "##### 使用方法\n",
    "\n",
    "- 将词典文件 `vocab.txt` 准备好，格式为每行一个词，使用该词典初始化 `MyTokenizer`。\n",
    "- 调用 `tokenize()`、`convert_tokens_to_ids()` 和 `convert_ids_to_tokens()` 等方法进行分词和ID转换操作。\n",
    "\n",
    "##### 其他功能扩展\n",
    "\n",
    "- **处理子词（Subword）**：如果希望支持子词分割，可以将 tokenizer 扩展为类似 BERT 的方式，使用 `WordPiece` 或 `Byte-Pair Encoding (BPE)` 等算法进一步细化分词。\n",
    "- **特殊Token支持**：可以添加更多特殊 token，如 `[CLS]`、`[SEP]` 等。\n",
    "\n",
    "这样，就能实现一个自定义的分词器，并且具备类似 GPT 和 BERT 的分词与 ID 转换功能，能够方便地在其它程序中导入和使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分词类\n",
    "import json\n",
    "class MyTokenizer:\n",
    "    def __init__(self, vocab_file):\n",
    "        # 加载词典\n",
    "        self.vocab = self.load_vocab(vocab_file)\n",
    "        self.reverse_vocab = {v: k for k, v in self.vocab.items()}  # ID 到 Token 的映射\n",
    "        self.unk_token = \"[UNK]\"  # 未知词标记\n",
    "        self.pad_token = \"[PAD]\"  # 填充标记\n",
    "    \n",
    "    @classmethod\n",
    "    def from_pretrained(cls, vocab_file):\n",
    "        return cls(vocab_file)\n",
    "    \n",
    "    def load_vocab(self, vocab_file):\n",
    "        \"\"\"从JSON文件加载词典\"\"\"\n",
    "        with open(vocab_file, 'r', encoding='utf-8') as f:\n",
    "            vocab = json.load(f)  # 从JSON文件读取数据并转换为 Python字典\n",
    "        return vocab\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        \"\"\"将输入文本进行分词\"\"\"\n",
    "        tokens = list(text)  # 将文本拆分成字符\n",
    "        output_tokens = []\n",
    "\n",
    "        for token in tokens:\n",
    "            # 如果token在词汇表中，直接添加\n",
    "            if token in self.vocab.keys():\n",
    "                output_tokens.append(token)\n",
    "            else:\n",
    "                # 否则，标记为[UNK]，代表这个词不在词汇表中\n",
    "                output_tokens.append(self.unk_token)\n",
    "        \n",
    "        return output_tokens\n",
    "\n",
    "    def convert_tokens_to_ids(self, tokens):\n",
    "        \"\"\"将token转换为ID\"\"\"\n",
    "        return [self.vocab.get(token, -1) for token in tokens]  # 返回对应的ID，如果没有找到则返回-1\n",
    "\n",
    "    def convert_ids_to_tokens(self, ids):\n",
    "        \"\"\"将ID转换为token\"\"\"\n",
    "        return [self.reverse_vocab.get(id, \"[UNK]\") for id in ids]  # 返回对应的token，如果没有找到则返回[UNK]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['好', '好', '学', '习', '，', '天', '天', '向', '上', '。', '维', '语', '版', '：', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', ' ', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]']\n",
      "Token IDs: [1099, 1099, 1203, 186, 5165, 1061, 1061, 700, 136, 119, 3519, 4213, 2814, 5166, -1, -1, -1, -1, -1, -1, -1, 1, -1, -1, -1, -1, -1, -1, -1]\n",
      "IDs to Tokens: ['好', '好', '学', '习', '，', '天', '天', '向', '上', '。', '维', '语', '版', '：', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', ' ', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]']\n"
     ]
    }
   ],
   "source": [
    "# 初始化分词器\n",
    "tokenizer = MyTokenizer.from_pretrained('data/news_vocab.json')\n",
    "# 分词操作\n",
    "sentence = \"好好学习，天天向上。维语版：جاپالىق ئۆگىنىڭ\"\n",
    "tokens = tokenizer.tokenize(sentence)\n",
    "print(\"Tokens:\", tokens)\n",
    "\n",
    "# 将tokens转换为ID\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(\"Token IDs:\", ids)\n",
    "\n",
    "# 将ID转换回Tokens\n",
    "reconstructed_tokens = tokenizer.convert_ids_to_tokens(ids)\n",
    "print(\"IDs to Tokens:\", reconstructed_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\u3000', '\\u3000', '新', '华', '社', '北', '京', '1', '2', '月', '3', '1', '日', '电', '\\xa0', '\\xa0', '国', '务', '院', '办']\n",
      "CPU times: total: 688 ms\n",
      "Wall time: 681 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 对整个语料库分词\n",
    "with open('./data/people.cn/news.txt', 'r', encoding='utf-8') as f:\n",
    "    corpus = f.read()\n",
    "tokens = tokenizer.tokenize(corpus)\n",
    "print(tokens[:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5519725"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs: [117, 117, 1971, 603, 3231, 583, 218, 16, 17, 2074, 18, 16, 1985, 2985, 81, 81, 921, 551, 4827, 548]\n"
     ]
    }
   ],
   "source": [
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(\"Token IDs:\", ids[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IDs to Tokens: ['\\u3000', '\\u3000', '新', '华', '社', '北', '京', '1', '2', '月', '3', '1', '日', '电', '\\xa0', '\\xa0', '国', '务', '院', '办']\n"
     ]
    }
   ],
   "source": [
    "# 将ID转换回Tokens\n",
    "id_to_tokens = tokenizer.convert_ids_to_tokens(ids)\n",
    "print(\"IDs to Tokens:\", id_to_tokens[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <b>2.1.3 划分数据集</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用其他分词工具，如GPT、BERT等分词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取语料库\n",
    "with open('./data/people.cn/news.txt', 'r', encoding='utf-8') as f:\n",
    "    corpus = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "词典大小为： 21128\n",
      "CPU times: total: 22.5 s\n",
      "Wall time: 33.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# 加载 BERT 中文词典\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(\"./BERT中文词典\")\n",
    "# 获取词典的大小（词典中包含的词的数量）\n",
    "vocab_size = len(bert_tokenizer)\n",
    "# 对整个语料库分词\n",
    "bert_tokens = bert_tokenizer.tokenize(corpus)\n",
    "# Token 转 ID\n",
    "data = bert_tokenizer.convert_tokens_to_ids(bert_tokens)\n",
    "print('词典大小为：', vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>准备数据集：将整个文本数据集 -> token ids数据集</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:\n",
      " tensor([3173, 1290, 4852, 1266,  776, 8110, 3299, 8176, 3189, 4510])\n",
      "IDs解码回文本:\n",
      " ['新', '华', '社', '北', '京', '12', '月', '31', '日', '电']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# Token 转 ID\n",
    "data = bert_tokenizer.convert_tokens_to_ids(bert_tokens)\n",
    "data = torch.tensor(data, dtype = torch.long)  # 数据集转换为 Pytorch 张量\n",
    "print(\"Token IDs:\\n\", data[:10])\n",
    "\n",
    "# ID # 解码回文本\n",
    "tokens_decoded = bert_tokenizer.convert_ids_to_tokens(data[:10].tolist())\n",
    "print(\"IDs解码回文本:\\n\", tokens_decoded[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "id": "f_WIXqxz0lU5",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集字符总数：4744016\n",
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 将数据集分为训练集和验证集两部分。训练集占比90%，验证集占比10%\n",
    "n_train = int(0.9*len(data)) #  计算训练集样本数\n",
    "print(f'训练集字符总数：{n_train}')\n",
    "train_data = data[:n_train]  # 之前的给训练集\n",
    "val_data = data[n_train:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TD5Bj8Y6IAD4",
    "outputId": "bf23c586-1d33-4af1-b63d-ce6f90b0a528"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3173, 1290, 4852, 1266,  776, 8110, 3299, 8176, 3189])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 段落文本最长值\n",
    "block_size = 8\n",
    "# 观察段落\n",
    "train_data[:block_size+1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从训练数据中截取一个文本块做为样本特征，文本块后面紧跟的下一个字词做为这个文本块的标签。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9HXDe8vGJCEn",
    "outputId": "588663aa-1de5-4ef7-aba0-4a96fe828353"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "样本 0 特征： tensor([3173])     标签： 1290\n",
      "样本 1 特征： tensor([3173, 1290])     标签： 4852\n",
      "样本 2 特征： tensor([3173, 1290, 4852])     标签： 1266\n",
      "样本 3 特征： tensor([3173, 1290, 4852, 1266])     标签： 776\n",
      "样本 4 特征： tensor([3173, 1290, 4852, 1266,  776])     标签： 8110\n",
      "样本 5 特征： tensor([3173, 1290, 4852, 1266,  776, 8110])     标签： 3299\n",
      "样本 6 特征： tensor([3173, 1290, 4852, 1266,  776, 8110, 3299])     标签： 8176\n",
      "样本 7 特征： tensor([3173, 1290, 4852, 1266,  776, 8110, 3299, 8176])     标签： 3189\n",
      "样本 0 特征： 新     标签： 华\n",
      "样本 1 特征： 新华     标签： 社\n",
      "样本 2 特征： 新华社     标签： 北\n",
      "样本 3 特征： 新华社北     标签： 京\n",
      "样本 4 特征： 新华社北京     标签： 12\n",
      "样本 5 特征： 新华社北京12     标签： 月\n",
      "样本 6 特征： 新华社北京12月     标签： 31\n",
      "样本 7 特征： 新华社北京12月31     标签： 日\n"
     ]
    }
   ],
   "source": [
    "# 看看一个文本块，可以组合出多少个训练样本\n",
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "# 基于ID的角度观察\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f'样本 {t} 特征： {context}     标签： {target}')\n",
    "# 基于文本的角度观察\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    context_token = bert_tokenizer.convert_ids_to_tokens(context)\n",
    "    target_token = bert_tokenizer.convert_ids_to_tokens([target])\n",
    "    print(f'样本 {t} 特征： {''.join(context_token)}     标签： {''.join(target_token)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <b>2.1.4 随机加载数据</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "Q3k1Czf7LuA9",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "4ea8e8a0-443c-49bb-b3bf-ba36e1712999"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "单批次样本特征：\n",
      "torch.Size([4, 8])\n",
      "tensor([[6772, 4960, 1139,  511, 1169, 5276, 3867, 6589],\n",
      "        [4545,  978, 2434,  510, 4495, 4289, 1093,  689],\n",
      "        [ 511, 1762, 1649, 1069, 8024, 4125, 4150,  837],\n",
      "        [3341, 8024,  704, 1744, 1066,  772, 1054, 2828]])\n",
      "===============================================================\n",
      "单批次样本标签：\n",
      "torch.Size([4, 8])\n",
      "tensor([[4960, 1139,  511, 1169, 5276, 3867, 6589, 2612],\n",
      "        [ 978, 2434,  510, 4495, 4289, 1093,  689,  510],\n",
      "        [1762, 1649, 1069, 8024, 4125, 4150,  837, 6853],\n",
      "        [8024,  704, 1744, 1066,  772, 1054, 2828, 1121]])\n",
      "样本特征： [6772] 标签: 4960\n",
      "样本特征： [6772, 4960] 标签: 1139\n",
      "样本特征： [6772, 4960, 1139] 标签: 511\n",
      "样本特征： [6772, 4960, 1139, 511] 标签: 1169\n",
      "样本特征： [6772, 4960, 1139, 511, 1169] 标签: 5276\n",
      "样本特征： [6772, 4960, 1139, 511, 1169, 5276] 标签: 3867\n",
      "样本特征： [6772, 4960, 1139, 511, 1169, 5276, 3867] 标签: 6589\n",
      "样本特征： [6772, 4960, 1139, 511, 1169, 5276, 3867, 6589] 标签: 2612\n",
      "样本特征： [4545] 标签: 978\n",
      "样本特征： [4545, 978] 标签: 2434\n",
      "样本特征： [4545, 978, 2434] 标签: 510\n",
      "样本特征： [4545, 978, 2434, 510] 标签: 4495\n",
      "样本特征： [4545, 978, 2434, 510, 4495] 标签: 4289\n",
      "样本特征： [4545, 978, 2434, 510, 4495, 4289] 标签: 1093\n",
      "样本特征： [4545, 978, 2434, 510, 4495, 4289, 1093] 标签: 689\n",
      "样本特征： [4545, 978, 2434, 510, 4495, 4289, 1093, 689] 标签: 510\n",
      "样本特征： [511] 标签: 1762\n",
      "样本特征： [511, 1762] 标签: 1649\n",
      "样本特征： [511, 1762, 1649] 标签: 1069\n",
      "样本特征： [511, 1762, 1649, 1069] 标签: 8024\n",
      "样本特征： [511, 1762, 1649, 1069, 8024] 标签: 4125\n",
      "样本特征： [511, 1762, 1649, 1069, 8024, 4125] 标签: 4150\n",
      "样本特征： [511, 1762, 1649, 1069, 8024, 4125, 4150] 标签: 837\n",
      "样本特征： [511, 1762, 1649, 1069, 8024, 4125, 4150, 837] 标签: 6853\n",
      "样本特征： [3341] 标签: 8024\n",
      "样本特征： [3341, 8024] 标签: 704\n",
      "样本特征： [3341, 8024, 704] 标签: 1744\n",
      "样本特征： [3341, 8024, 704, 1744] 标签: 1066\n",
      "样本特征： [3341, 8024, 704, 1744, 1066] 标签: 772\n",
      "样本特征： [3341, 8024, 704, 1744, 1066, 772] 标签: 1054\n",
      "样本特征： [3341, 8024, 704, 1744, 1066, 772, 1054] 标签: 2828\n",
      "样本特征： [3341, 8024, 704, 1744, 1066, 772, 1054, 2828] 标签: 1121\n",
      "样本 0 特征： 较     标签： 突\n",
      "样本 1 特征： 较突     标签： 出\n",
      "样本 2 特征： 较突出     标签： 。\n",
      "样本 3 特征： 较突出。     标签： 制\n",
      "样本 4 特征： 较突出。制     标签： 约\n",
      "样本 5 特征： 较突出。制约     标签： 消\n",
      "样本 6 特征： 较突出。制约消     标签： 费\n",
      "样本 7 特征： 较突出。制约消费     标签： 恢\n",
      "样本 0 特征： 疗     标签： 健\n",
      "样本 1 特征： 疗健     标签： 康\n",
      "样本 2 特征： 疗健康     标签： 、\n",
      "样本 3 特征： 疗健康、     标签： 生\n",
      "样本 4 特征： 疗健康、生     标签： 物\n",
      "样本 5 特征： 疗健康、生物     标签： 农\n",
      "样本 6 特征： 疗健康、生物农     标签： 业\n",
      "样本 7 特征： 疗健康、生物农业     标签： 、\n",
      "样本 0 特征： 。     标签： 在\n",
      "样本 1 特征： 。在     标签： 嘉\n",
      "样本 2 特征： 。在嘉     标签： 兴\n",
      "样本 3 特征： 。在嘉兴     标签： ，\n",
      "样本 4 特征： 。在嘉兴，     标签： 火\n",
      "样本 5 特征： 。在嘉兴，火     标签： 炬\n",
      "样本 6 特征： 。在嘉兴，火炬     标签： 传\n",
      "样本 7 特征： 。在嘉兴，火炬传     标签： 递\n",
      "样本 0 特征： 来     标签： ，\n",
      "样本 1 特征： 来，     标签： 中\n",
      "样本 2 特征： 来，中     标签： 国\n",
      "样本 3 特征： 来，中国     标签： 共\n",
      "样本 4 特征： 来，中国共     标签： 产\n",
      "样本 5 特征： 来，中国共产     标签： 党\n",
      "样本 6 特征： 来，中国共产党     标签： 把\n",
      "样本 7 特征： 来，中国共产党把     标签： 减\n"
     ]
    }
   ],
   "source": [
    "random_seed = 2025 # 随机数种子\n",
    "torch.manual_seed(random_seed)\n",
    "batch_size = 4 # 批处理样本数\n",
    "block_size = 8 # 最大序列长度\n",
    "\n",
    "# 从数据集中随机抽取一批样本, 返回文本块和标签\n",
    "def get_batch(dataset, batch_size=4, block_size=16, device='cpu'):\n",
    "    # 根据随机样本的抽取范围和批次大小随机抽取，返回每个文本块的起始索引\n",
    "    ix = torch.randint(len(dataset) - block_size, (batch_size,))\n",
    "    # 把本批次的文本块堆叠在一起，形成文本矩阵\n",
    "    x = torch.stack([dataset[i:i + block_size] for i in ix], dim=0)  # 按行堆叠\n",
    "    # 标签堆叠在一起，形成标签矩阵\n",
    "    y = torch.stack([dataset[i + 1:i + block_size + 1] for i in ix], dim=0)\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "batchX, batchY = get_batch(train_data, batch_size = 4, block_size = 8, device='cpu')\n",
    "print(f'单批次样本特征：\\n{batchX.shape}\\n{batchX}')\n",
    "print('===============================================================')\n",
    "print(f'单批次样本标签：\\n{batchY.shape}\\n{batchY}')\n",
    "# 遍历批次样本\n",
    "for i in range(batch_size): # 样本维度，第 i 个样本\n",
    "    for t in range(block_size): # 序列长度维度，t表示token顺序\n",
    "        context = batchX[i, :t+1]\n",
    "        target = batchY[i, t]\n",
    "        print(f'样本特征： {context.tolist()} 标签: {target}')\n",
    "# 基于文本的角度观察\n",
    "for i in range(batch_size): # 样本维度，第 i 个样本\n",
    "    for t in range(block_size):\n",
    "        context = batchX[i, :t+1]\n",
    "        target = batchY[i, t]\n",
    "        context_token = bert_tokenizer.convert_ids_to_tokens(context)\n",
    "        target_token = bert_tokenizer.convert_ids_to_tokens([target])\n",
    "        print(f'样本 {t} 特征： {''.join(context_token)}     标签： {''.join(target_token)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qpyyAeIzQjlO",
    "outputId": "a650f8dc-da81-400b-bc59-0a595487fdb9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6772, 4960, 1139,  511, 1169, 5276, 3867, 6589],\n",
      "        [4545,  978, 2434,  510, 4495, 4289, 1093,  689],\n",
      "        [ 511, 1762, 1649, 1069, 8024, 4125, 4150,  837],\n",
      "        [3341, 8024,  704, 1744, 1066,  772, 1054, 2828]])\n",
      "tensor([[4960, 1139,  511, 1169, 5276, 3867, 6589, 2612],\n",
      "        [ 978, 2434,  510, 4495, 4289, 1093,  689,  510],\n",
      "        [1762, 1649, 1069, 8024, 4125, 4150,  837, 6853],\n",
      "        [8024,  704, 1744, 1066,  772, 1054, 2828, 1121]])\n"
     ]
    }
   ],
   "source": [
    "print(batchX) # 这是我们要送给模型的数据结构\n",
    "print(batchY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"red\">加载数据集</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "批次 1:\n",
      "输入序列： tensor([[ 671, 2399,    0,    0,    0,    0,    0,    0],\n",
      "        [1962, 4638, 3302,    0,    0,    0,    0,    0],\n",
      "        [7566, 1818, 4638,  924, 7397,  510, 2190, 1765],\n",
      "        [3198, 3309,  511, 1059,    0,    0,    0,    0]])\n",
      "标签： tensor([8024, 1218, 3175, 1054])\n",
      "\n",
      "批次 2:\n",
      "输入序列： tensor([[7357, 1300, 4706,  704, 8024,    0,    0,    0],\n",
      "        [4757, 4688, 6444, 6237, 1168,  721, 1218, 2193],\n",
      "        [ 758, 2399, 6121, 1220, 3175,    0,    0,    0],\n",
      "        [2769, 3918, 2697, 5783, 2401,    0,    0,    0]])\n",
      "标签： tensor([4384, 3952, 3428,  511])\n",
      "\n",
      "批次 3:\n",
      "输入序列： tensor([[1912, 6598, 6847, 1232, 1872,    0,    0,    0],\n",
      "        [1147,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [8024, 2501,    0,    0,    0,    0,    0,    0],\n",
      "        [ 772,  689, 1394,    0,    0,    0,    0,    0]])\n",
      "标签： tensor([7270, 2617, 2768,  868])\n",
      "\n",
      "批次 4:\n",
      "输入序列： tensor([[6206, 3724, 8024,    0,    0,    0,    0,    0],\n",
      "        [6581,  833,  510,    0,    0,    0,    0,    0],\n",
      "        [5018,  676, 2399, 1762,    0,    0,    0,    0],\n",
      "        [3300,  749, 5632, 2346, 4638, 6639, 4413, 1767]])\n",
      "标签： tensor([2828, 2458, 4495, 8024])\n",
      "\n",
      "批次 5:\n",
      "输入序列： tensor([[2339,  868, 5287, 1057,    0,    0,    0,    0],\n",
      "        [1068, 3175, 7481, 8024, 1217, 1920,    0,    0],\n",
      "        [7032,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [ 758, 1282, 7027, 8024,  680, 7392, 1880, 4638]])\n",
      "标签： tensor([3777, 1291, 1970, 7987])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "# 重新定义 TextDataset 类\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, input_ids, labels):\n",
    "        self.input_ids = input_ids\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.labels[idx]\n",
    "\n",
    "# 加载保存的 Dataset 数据集\n",
    "loaded_dataset = torch.load('data/text_dataset.pt', weights_only=False)\n",
    "\n",
    "# 使用 DataLoader 进行批量加载\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataloader = DataLoader(loaded_dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "# 测试加载的 DataLoader\n",
    "for batch_idx, (batchX, batchY) in enumerate(dataloader):\n",
    "    print(f\"批次 {batch_idx + 1}:\")\n",
    "    print(\"输入序列：\", batchX)\n",
    "    print(\"标签：\", batchY)\n",
    "    print()\n",
    "    if batch_idx + 1 >= 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"red\"><b>定义模型</b></font>\n",
    "- 参照Transformer论文给出的结构分步定义。\n",
    "- 从解码器输入层的词向量编码入手\n",
    "- 到解码器输出层的文本生成结束。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align = \"center\"><img width = \"325\" height = \"500\" src = \"./images/gpt_structure.jpg\" /></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=\"red\"><b>2.1.5 GPT输入与输出</b></font>\n",
    "- 定义类GPT，先搭个框架，只包含一个输入层，一个线性输出层。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class GPT(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size: int, d_model: int):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model) # 定义输入层\n",
    "        self.output = nn.Linear(d_model, vocab_size, bias=False)  # 定义输出层\n",
    "        # 输出层与词嵌入层共享参数，这和 LLama、通义千问等开源模型类似\n",
    "        # self.embedding.weight = self.output.weight \n",
    "        \n",
    "    def forward(self, idx):\n",
    "        # (batch, seq_len) --> (batch, seq_len, d_model)\n",
    "        idx = idx.long()  # 将索引转换为 LongTensor\n",
    "        idx = self.embedding(idx)\n",
    "        # (batch, seq_len, d_model) --> (batch, seq_len, vocab_size)\n",
    "        logits = self.output(idx)      \n",
    "        return logits\n",
    "\n",
    "    # 模型自回归持续推理，生成长度为 max_new_tokens 的文本块\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx 的维度为： (batch, seq_len) ，表示已知的token id 序列\n",
    "        # max_new_tokens表示新生成的文本的最大长度\n",
    "        for _ in range(max_new_tokens):\n",
    "            # 模型正向传播\n",
    "            logits = self(idx)  \n",
    "            # 选择每个批次中序列的最后一个时间步\n",
    "            logits = logits[:, -1, :] # (batch, seq_len, vocab_size)->(batch, vocab_size)\n",
    "            # 沿着最后一维进行 softmax 归一化，将原始得分转换为概率分布\n",
    "            probs = F.softmax(logits, dim=-1) # (batch, vocab_size)\n",
    "            # 根据概率分布probs采样下一个字词的索引\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (batch, 1)\n",
    "            # 追加到已有的字词序列中，将两个张量沿着指定的维度连接起来\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (batch, seq_len+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8, 21128])\n",
      "生成长度为100字的短文：\n",
      "[0, 8105, 2815, 13985, 3311, 15458, 3245, 5964, 19362, 16657, 6737, 1259, 6305, 5829, 2990, 11872, 11458, 19821, 4501, 6649, 19755, 2180, 6374, 173, 10462, 12088, 15130, 19384, 5775, 16186, 20738, 15333, 930, 2954, 5705, 21017, 15703, 1776, 9718, 42, 5755, 15189, 4350, 9485, 8471, 871, 3833, 10127, 1864, 4395, 17460, 10335, 18602, 13328, 16208, 6266, 18247, 9749, 1284, 17809, 17400, 18507, 1141, 12975, 11393, 17637, 15039, 1478, 2343, 18749, 13217, 16470, 10414, 5277, 874, 8969, 18264, 763, 7927, 6509, 18241, 14081, 19604, 10266, 28, 3296, 16210, 20109, 19157, 2935, 15692, 540, 20097, 12673, 7877, 9081, 9334, 13133, 4297, 1243, 5437]\n",
      "[PAD]😎扮信朧幸晤藉読檸輔包読菖提suelog轰甩跌蹦寧讨£835tiger嫣諷荔敎馮岱俩掃芬鼐悪坏www[unused42]茫宓猾ettoday30佟活pmi墉珉珠asr脅,斃詐絶red卅瞿猪耗函originalsppi痠妩咘巩芋mobile校est级佣style綴些麗貝絨儆賤jason[unused28]替斋鈺蟲捐悌お釵364鷹139nasa432犀務翼\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# 加载 BERT 中文词典\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(\"./BERT中文词典\")\n",
    "\n",
    "# 检测设备\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "vocab_size = 21128\n",
    "d_model = 512\n",
    "gpt = GPT(vocab_size, d_model).to(device) # 实例化模型\n",
    "\n",
    "batchX = batchX.to(device)\n",
    "logits = gpt(batchX)  # 推理 （注意，此时模型还没有被训练！！！）\n",
    "print(logits.shape)\n",
    "# 用这个还没有被训练过的模型写一篇100字的资讯文稿,初始输入是一个[[0]]token\n",
    "idx = torch.zeros((1, 1), dtype=torch.long).to(device)\n",
    "generate_text_ids = gpt.generate(idx, max_new_tokens=100)[0].tolist()\n",
    "generate_text = bert_tokenizer.decode(generate_text_ids).replace(\" \", \"\")\n",
    "print(\"生成长度为100字的短文：\")\n",
    "print(generate_text_ids)\n",
    "print(generate_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "         Embedding-1              [-1, 64, 512]      10,817,536\n",
      "            Linear-2            [-1, 64, 21128]      10,817,536\n",
      "================================================================\n",
      "Total params: 21,635,072\n",
      "Trainable params: 21,635,072\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 10.57\n",
      "Params size (MB): 82.53\n",
      "Estimated Total Size (MB): 93.10\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary # pip install torchsummary\n",
    "summary(gpt, (64,))   # 输入形状应该是 (seq_len, ),即文本的最大长度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "参数名称: embedding.weight, 参数维度: torch.Size([21128, 512])\n",
      "参数名称: output.weight, 参数维度: torch.Size([21128, 512])\n",
      "模型参数总量: 21635072\n"
     ]
    }
   ],
   "source": [
    "# 打印所有可训练参数\n",
    "for name, param in gpt.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"参数名称: {name}, 参数维度: {param.shape}\")\n",
    "# 所有参数的总数\n",
    "total_params = sum(p.numel() for p in gpt.parameters())\n",
    "print(f\"模型参数总量: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "eTyJ8qAaDdiF"
   },
   "outputs": [],
   "source": [
    "# 优化算法\n",
    "optimizer = torch.optim.AdamW(gpt.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hs4kI8YdEkQj",
    "outputId": "42ded55c-2983-4d91-c528-675b2edfa849"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "损失值： 8.147138595581055\n",
      "生成长度为100字的短文：\n",
      "[0, 18259, 514, 7834, 14000, 7306, 11822, 7255, 11911, 16053, 6185, 6416, 16017, 11202, 10162, 4950, 14487, 14641, 16631, 17473, 17014, 5533, 20504, 12140, 11380, 3477, 20070, 12370, 19748, 14862, 5283, 9286, 19853, 14306, 20497, 8621, 10125, 11029, 12642, 3380, 11493, 4963, 9347, 11137, 9763, 9514, 15948, 10742, 16412, 15293, 5645, 16639, 7626, 3879, 6755, 1326, 19857, 13610, 9732, 18632, 19645, 16595, 13869, 3414, 17452, 9743, 1035, 6737, 17254, 4990, 11150, 20484, 15179, 11932, 119, 12564, 10459, 11737, 18588, 8609, 2764, 17789, 14691, 17571, 20267, 19062, 7354, 12890, 1540, 17122, 15420, 16568, 2690, 20897, 6595, 14511, 18749, 16514, 12902, 12029, 1180]\n",
      "[PAD]綫〇鲤個闪っています镍moneydj揣複诠掐du▲topapr穗吹喀樽琇湊胫霆nana2gb棟醣advanced蹋垦纯hot辩勤雷maxjust866yy柒780窈ainionschegov拳costco枇屉與機餘涠轟厉辯△image腎贸槃们栩珉74儼輔然竖edit雙孿867.lovestationious胥no戌瞇嗦画铰虻际day4唔潼帑業愉鲷赁呱芋梦cdmaいを剉\n",
      "CPU times: total: 641 ms\n",
      "Wall time: 743 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for steps in range(100): # 增加迭代步数效果可能会更好\n",
    "\n",
    "    # 随机抽取一个批次样本\n",
    "    batchX, batchY = get_batch(train_data, batch_size = 4, block_size = 16, device = device)\n",
    "\n",
    "    batchX, batchY = batchX.to(device), batchY.to(device)\n",
    "    logits = gpt(batchX)\n",
    "    batch, seq_len, d_model = logits.shape\n",
    "    logits = logits.view(batch*seq_len, d_model)\n",
    "    batchY = batchY.view(batch*seq_len)\n",
    "    \n",
    "    loss = F.cross_entropy(logits, batchY)\n",
    "    optimizer.zero_grad(set_to_none=True) # 累积梯度清零\n",
    "    loss.backward()  # 反向传播计算梯度\n",
    "    optimizer.step()  # 优化算法更新参数\n",
    "print('损失值：',loss.item())\n",
    "# 用这个带优化算法并训练过100步的模型写一篇100字的资讯文稿,初始输入是一个[[0]]token\n",
    "idx = torch.zeros((1, 1), dtype=torch.long).to(device)\n",
    "generate_text_ids = gpt.generate(idx, max_new_tokens=100)[0].tolist()\n",
    "generate_text = bert_tokenizer.decode(generate_text_ids).replace(\" \", \"\")\n",
    "print(\"生成长度为100字的短文：\")\n",
    "print(generate_text_ids)\n",
    "print(generate_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EcVIDWAZEtjN",
    "outputId": "0ad6f9d2-ad58-4498-a5f8-6f31407bb18b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PAD]茜俎跨獸悩耋矿達蚊ding厅棋tablecontinue赫30gさ吞穴雑dit辇竇稿233妥戲疆幔舺▶顶摸咏嗷礼犄nvidia脯锋103隕雛匮奸繕kic彙ر⑤雄也钿興繪熄焘拚燧into綱16g膘energy盖摧tokyolines僕離玥ц嘶鸡倦嗲le垩余物よって搏赘酰祐始331群農退堆嬢ping竞lon瑟bay瓤屬厚\n"
     ]
    }
   ],
   "source": [
    "idx = torch.zeros((1, 1), dtype=torch.long).to(device)\n",
    "generate_text_ids = gpt.generate(idx, max_new_tokens=100)[0].tolist()\n",
    "generate_text = bert_tokenizer.decode(generate_text_ids).replace(\" \", \"\")\n",
    "print(generate_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=\"red\"><b>2.1.6 增加位置编码层</b></font>\n",
    "- 类似于第一章Transformer中的正余弦绝对位置编码，这里将GPT的位置编码层定义为正余弦绝对位置编码。现在更多大模型采用相对位置编码。\n",
    "- 修改GPT类，添加位置编码层。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "PE(pos, 2i) = \\sin\\left(\\frac{\\text{pos}}{10000^{\\frac{2i}{d_{\\text{model}}}}}\\right)\n",
    "$$\n",
    "$$\n",
    "PE(pos, 2i+1) = \\cos\\left(\\frac{\\text{pos}}{10000^{\\frac{2i}{d_{\\text{model}}}}}\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 修改GPT类，增加位置编码层："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 位置编码\n",
    "class PositionEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, max_len: int = 1024, d_model: int = 512):\n",
    "        super().__init__()\n",
    "\n",
    "        # 初始化一个零矩阵 pe，形状为 (seq_len, d_model)，用于存储位置编码\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "\n",
    "        # 生成从 0 到 max_len-1 的整数序列，表示位置索引。将其变为列向量，形状为 (max_lenn, 1)     \n",
    "        position = torch.arange(start=0, end=max_len, step=1).float().unsqueeze(1)\n",
    "\n",
    "        ## 位置编码计算公式：\n",
    "        ## PE(pos, 2i)   = sin(pos / 10000^(2i/d_model))\n",
    "        ## PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))\n",
    "\n",
    "        embedding_index = torch.arange(start=0, end=d_model, step=2).float()\n",
    "\n",
    "        ## 计算频率缩放因子\n",
    "        div_term = 1 / torch.tensor(10000.0) ** (embedding_index / d_model)\n",
    "\n",
    "        # 应用正弦和余弦函数计算位置编码\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)  \n",
    "        pe[:, 1::2] = torch.cos(position * div_term) \n",
    "        pe = pe.unsqueeze(0)  # 在第0维添加一个维度，变成 (1, max_len, d_model)\n",
    "\n",
    "        ## 注册位置编码矩阵\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "        # 词向量叠加位置编码向量\n",
    "\n",
    "    def forward(self, word_embeddings):\n",
    "        return word_embeddings + self.pe[:, :word_embeddings.size(1), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class GPT(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size: int, d_model: int):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model) # 定义输入层\n",
    "        self.position = PositionEncoding(d_model = d_model)  # 定义位置编码层\n",
    "        self.output = nn.Linear(d_model, vocab_size)  # 定义输出层\n",
    "        # 输出层与词嵌入层共享参数，这和 LLama、通义千问等开源模型类似\n",
    "        # self.embedding.weight = self.output.weight \n",
    "        \n",
    "        \n",
    "    def forward(self, idx):\n",
    "        # (batch, seq_len) --> (batch, seq_len, d_model)\n",
    "        idx = idx.long()  # 将索引转换为 LongTensor\n",
    "        idx = self.embedding(idx)\n",
    "        idx = self.position(idx)\n",
    "        logits = self.output(idx)\n",
    "       \n",
    "        return logits\n",
    "\n",
    "    # 模型自回归持续推理，生成长度为 max_new_tokens 的文本块\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx 的维度为： (batch, seq_len)\n",
    "        # max_new_tokens表示新生成的文本的最大长度\n",
    "        for _ in range(max_new_tokens):\n",
    "            # 模型正向传播\n",
    "            logits = self(idx)  \n",
    "            # 选择每个批次中序列的最后一个时间步\n",
    "            logits = logits[:, -1, :] # (batch, seq_len, d_model)->(batch,  d_model)\n",
    "            # 沿着最后一维进行 softmax 归一化，将原始得分转换为概率分布\n",
    "            probs = F.softmax(logits, dim=-1) # (batch, d_model)\n",
    "            # 根据概率分布 probs 采样下一个字词的索引\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (batch, 1)\n",
    "            # 追加到已有的字词序列中，将两个张量沿着指定的维度（第二个维度，时间步维度）连接起来\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (batch, seq_len+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 测试："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "生成长度为100字的短文：\n",
      "[0, 13522, 6542, 16246, 14375, 8618, 11199, 5604, 11303, 13425, 20096, 3089, 16767, 422, 20005, 274, 2349, 17999, 17987, 17039, 18323, 12292, 13344, 7857, 10863, 272, 13289, 2494, 18129, 1171, 14177, 19326, 10, 12353, 15943, 7924, 1533, 17229, 6169, 5616, 12621, 14659, 9151, 644, 18281, 19322, 3738, 17040, 20993, 17029, 21107, 6537, 1931, 16152, 3035, 20147, 6537, 20664, 3988, 302, 17458, 8936, 4447, 782, 310, 18085, 11110, 7436, 19103, 852, 17058, 6911, 19450, 15786, 777, 8315, 1535, 2353, 11513, 18817, 17076, 2, 15595, 12172, 15839, 7131, 2519, 13293, 15505, 9859, 9988, 15267, 13709, 2672, 8904, 17026, 13048, 3767, 6498, 21110, 13258]\n",
      "[PAD]ⅳ 賞日卷のてties 膚 moь釧 擠氨 ⒊郑 ي 巴稽稞溧纍 009{ 鴉 paper ه kl 彎篩 券凍詛 [unused10]cms括 麋 唄烦 裘 膾walk喧 japan ・締詆 污溪麺溅･ 賈 夹擱 摔錚 賈飞 溶 ᄑ珙ent 瑜 人 ᅨ筐o2 雰蜆 但滕 遽访憎 亭al 唇 巾 insee茲滸 [unused2]徠 5k戰 鏽 征 california廟 しく800尕ㄋ 惧02湿 some 沢 豪ｲgion\n"
     ]
    }
   ],
   "source": [
    "d_model = 512\n",
    "max_len = 8\n",
    "gpt = GPT(vocab_size, d_model).to(device) # 实例化模型\n",
    "logits = gpt(batchX)  # 推理\n",
    "\n",
    "# 用这个还没有被训练过的模型写一篇100字的资讯文稿,初始输入是一个[[0]]token\n",
    "idx = torch.zeros((1, 1), dtype=torch.long).to(device)\n",
    "generate_text_ids = gpt.generate(idx, max_new_tokens=100)[0].tolist()\n",
    "generate_text = bert_tokenizer.decode(generate_text_ids)\n",
    "print(\"生成长度为100字的短文：\")\n",
    "print(generate_text_ids)\n",
    "print(generate_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=\"red\"><b>2.1.7 多头自注意力层</b></font>\n",
    "- 多头自注意力（Multi-Head Self-Attention） 的计算逻辑，主要由两个部分组成：\n",
    "- 一个是单个头的注意力计算（Head 类）：包含了对查询（Query）、键（Key）、值（Value）的计算。\n",
    "- 另一个是多头注意力的并行计算（MultiHeadAttention 类）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**注意力机制（Attention Mechanism）** 的核心思想是：在处理一个输入序列时，模型应当根据每个位置的上下文信息来加权地“关注”序列中的其他位置，而不是简单地依赖固定的上下文窗口。<br/>自注意力（Self-Attention）则是让每个输入的位置都可以与其他所有位置进行交互，计算它们之间的关系。\n",
    "\n",
    "#####  **基本的注意力计算公式：**\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right) V\n",
    "$$\n",
    "\n",
    "其中：\n",
    "- \\( Q \\)：查询（Query）矩阵，是输入序列中每个位置的表示，通过线性变换得到。查询决定了我们想要关注哪些位置。<br/>\n",
    "- \\( K \\)：键（Key）矩阵，是输入序列中每个位置的表示，通过线性变换得到。键决定了我们应该关注哪些位置。\n",
    "- \\( V \\)：值（Value）矩阵，是输入序列中每个位置的表示，通过线性变换得到。值包含了最终实际的内容。\n",
    "- \\( d_k \\)：是键（Key）向量的维度，通常与查询（Query）向量的维度相同。它用于缩放点积计算的结果。\n",
    "\n",
    "- **softmax**：将关联度转化为概率，表示查询对每个位置的关注程度。\n",
    "- **加权求和**：根据关注程度对值进行加权，得到最终的包含注意力的输出。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 单头注意力计算\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, d_model, head_size, block_size, dropout):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(d_model, head_size, bias=False)\n",
    "        self.query = nn.Linear(d_model, head_size, bias=False)\n",
    "        self.value = nn.Linear(d_model, head_size, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, padding_mask=None):\n",
    "        batch, seq_len, d_model = x.shape\n",
    "        # 线性变换得到 K 值\n",
    "        K = self.key(x)   # (batch, seq_len, head_size)\n",
    "        # 线性变换得到 Q 值\n",
    "        Q = self.query(x) # (batch, seq_len, head_size)\n",
    "        # 计算注意力得分\n",
    "        # (batch, seq_len, head_size)->(batch, seq_len, seq_len)\n",
    "        wei = Q @ K.transpose(-2,-1) *d_model**-0.5 \n",
    "        \n",
    "        # 因果掩码：保证每个位置只能看到当前和之前的 token\n",
    "        # (batch, seq_len, seq_len)\n",
    "        mask = torch.tril(torch.ones((1, seq_len, seq_len))).to(x.device)\n",
    "        wei = wei.masked_fill( mask == 0, float('-inf'))\n",
    "        # 通过 softmax 函数将注意力得分归一化，使得每个查询的注意力得分变成一个概率分布\n",
    "        \n",
    "        # 填充掩码：防止填充位置参与注意力计算\n",
    "        if padding_mask is not None:\n",
    "            padding_mask = padding_mask.unsqueeze(1).unsqueeze(2)  # (batch, 1, 1, seq_len)\n",
    "            wei = wei.masked_fill(padding_mask == 0, float('-inf'))  # 应用填充掩码\n",
    "        \n",
    "        wei = F.softmax(wei, dim=-1) # (batch, seq_len, seq_len)\n",
    "        wei = self.dropout(wei)\n",
    "\n",
    "        # 线性变换得到 V 值\n",
    "        V = self.value(x) # (batch, seq_len, head_size)\n",
    "        # wei @ v 是对值（Value）的加权求和，得到最终的输出 out，形状为 (batch, seq_len, head_size)\n",
    "        # (batch, seq_len, seq_len) @ (batch, seq_len, head_size) -> (batch, seq_len, head_size)\n",
    "        out = wei @ V  \n",
    "        return out\n",
    "\n",
    "# 多个注意力头并行计算，然后将其结果合并\n",
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, num_heads, block_size, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        # 通过 ModuleList 创建多个 Head 实例，每个头的大小为 head_size，共有 num_heads 个头\n",
    "        # ModuleList 允许将这些子模块（即多个头）一起管理\n",
    "        head_size = d_model // num_heads\n",
    "        self.heads = nn.ModuleList([Head(d_model, head_size, block_size, dropout) for _ in range(num_heads)])\n",
    "       \n",
    "        # 将多个注意力头的输出拼接起来后映射回原始的嵌入空间\n",
    "        self.proj = nn.Linear(num_heads * head_size, num_heads * head_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 对每个注意力头 h(x) 计算得到的输出进行拼接，拼接的维度是最后一维（dim=-1）。\n",
    "        # 每个头的输出 (batch, seq_len, head_size) \n",
    "        # 会被拼接成一个更大的张量 (batch, seq_len, num_heads * head_size)\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=\"red\"><b>2.1.8 增加前馈网络层</b></font>\n",
    "- 前馈网络由两层组成：第一层包含 d_model*4 个神经元，第二层包含d_model个神经元。\n",
    "- 第一层带激活函数ReLU，第二层无激活函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedFoward(nn.Module):\n",
    "    def __init__(self, d_model, dropout):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(d_model, 4 * d_model),\n",
    "            nn.GELU(),    # GPT3由ReLU替换为GeLU\n",
    "            nn.Linear(4 * d_model, d_model),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=\"red\"><b>2.1.9 增加解码器层</b></font>\n",
    "- 类DecoderBlock实现解码器单层结构的定义，包含带掩码的多头注意力模块，前馈网络。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, block_size, dropout):\n",
    "        # d_model: 词向量长度, num_heads: 注意力头数, block_size:最大文本块长度\n",
    "        super().__init__()\n",
    "        self.ma = MultiHeadAttention(d_model, num_heads, block_size, dropout)\n",
    "        self.ffwd = FeedFoward(d_model, dropout)\n",
    "        self.ln1 = nn.LayerNorm(d_model)  # 层标准化\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.ma(self.ln1(x))  # 此处先标准化，再计算多头注意力\n",
    "        x = x + self.ffwd(self.ln2(x)) # 先层标准化，再前馈网络\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=\"red\"><b>2.1.10 GPT模型完整定义</b></font>\n",
    "- 基于Transformer解码器实现\n",
    "- 基于位置编码类、单头注意力类、多头注意力类、前馈网络、单层解码器类实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class GPT(nn.Module):\n",
    "\n",
    "    def __init__(self, \n",
    "                 vocab_size: int, \n",
    "                 d_model: int = 512, \n",
    "                 n_layer: int = 6, \n",
    "                 n_head: int = 8, \n",
    "                 block_size:int = 16,\n",
    "                 dropout:int = 0.1):\n",
    "        super().__init__()\n",
    "        self.block_size = block_size\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model) # 定义输入层\n",
    "        self.position = PositionEncoding(d_model = d_model)  # 定义位置编码层       \n",
    "        self.blocks = nn.Sequential(*[DecoderBlock(d_model, \n",
    "                                                   num_heads=n_head, \n",
    "                                                   block_size=block_size, dropout = dropout) \\\n",
    "                                      for _ in range(n_layer)])\n",
    "        self.ln = nn.LayerNorm(d_model) # 输出层标准化\n",
    "        self.output = nn.Linear(d_model, vocab_size)  # 定义输出层\n",
    "        # 输出层与词嵌入层共享参数，这和 LLama、通义千问等开源模型类似\n",
    "        # self.embedding.weight = self.output.weight\n",
    "\n",
    "    def forward(self, idx):       \n",
    "        idx = idx.long()  # 将索引转换为 LongTensor\n",
    "        # (batch, seq_len) --> (batch, seq_len, d_model)\n",
    "        x = self.embedding(idx)\n",
    "        x = self.position(x)\n",
    "        x = self.blocks(x)   #  (batch, seq_len, d_model)\n",
    "        x = self.ln(x)   #  (batch, seq_len, d_model)\n",
    "        logits = self.output(x)   # (batch, seq_len,,vocab_size)\n",
    "\n",
    "        return logits\n",
    "              \n",
    "    # 模型自回归持续推理，生成长度为 max_new_tokens 的文本块\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx 的维度为： (batch, seq_len) ，表示当前上下文token在词典中的索引（indices）\n",
    "        # max_new_tokens表示新生成的文本的最大长度\n",
    "        for _ in range(max_new_tokens):\n",
    "            # 切片取出每个样本最后 block_size 个 tokens 元素\n",
    "            idx_cond = idx[:, -self.block_size:]\n",
    "            # 模型正向传播\n",
    "            logits = self(idx_cond)  \n",
    "            # 选择每个批次中序列的最后一个时间步\n",
    "             # (batch, seq_len, d_model)->(batch, d_model)\n",
    "            logits = logits[:, -1, :]\n",
    "            # 沿着最后一维进行 softmax 归一化，将原始得分转换为概率分布\n",
    "            probs = F.softmax(logits, dim=-1) # (batch, d_model)\n",
    "            # 根据概率分布probs采样下一个字词的索引\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (batch,1)\n",
    "            # 追加到已有的字词序列中，将两个张量沿着指定的维度连接起来\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (batch, seq_len+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "生成长度为100字的短文：\n",
      "[0, 18053, 9044, 9228, 19952, 7706, 9761, 1861, 20906, 13106, 15729, 11711, 15362, 21017, 5323, 9162, 17851, 17681, 14002, 10251, 10652, 9688, 4203, 2545, 14813, 5666, 5973, 3071, 18826, 19877, 16557, 17840, 19227, 18587, 17807, 6290, 3075, 13951, 6669, 9523, 16649, 15013, 14208, 8239, 20182, 11918, 18982, 7397, 14762, 16757, 1938, 9253, 4237, 2487, 12701, 4757, 7310, 17760, 8544, 9915, 5411, 2055, 15762, 20664, 20995, 15509, 1462, 9787, 18123, 13684, 32, 4689, 15362, 16990, 12153, 10262, 2106, 5219, 15148, 5103, 5345, 7726, 15944, 19441, 14664, 9912, 13123, 2651, 10128, 11428, 12263, 6043, 9872, 11443, 18884, 17469, 5869, 20979, 16791, 18484, 5957]\n",
      "[PAD]竣260press遠驊cs墀鳗beautiful惧nand崗鼐绣ct础癣倍bayx86hktvmall煎徳圖舵藥撻荆还楂砥裙胤瞻誌擁侑踉1935檐奮刎42鏘miacare蔷障嚐気奉1947燉强arpg矛闯眷881919羨媚慈飞麼廬命2030篓ケ[unused32]省崗渐するとあなたにもっとマッチしたisland孟緒嬿籼缀驸拭讴喰smithcm～悴802101vipnsis蜀meikiいるのて菏現董鹿汝翟薨\n"
     ]
    }
   ],
   "source": [
    "# 检测设备\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "d_model = 512\n",
    "vocab_size = 21128\n",
    "gpt = GPT(vocab_size, d_model).to(device) # 实例化模型\n",
    "# batchX = batchX.to(device)\n",
    "# logits = gpt(batchX)  # 推理 （注意，此时模型还没有被训练！！！）\n",
    "# 用这个还没有被训练过的模型写一篇100字的资讯文稿,初始输入是一个[[0]]token\n",
    "idx = torch.zeros((1, 1), dtype=torch.long).to(device)\n",
    "generate_text_ids = gpt.generate(idx, max_new_tokens=100)[0].tolist()\n",
    "generate_text = bert_tokenizer.decode(generate_text_ids).replace(\" \", \"\")\n",
    "print(\"生成长度为100字的短文：\")\n",
    "print(generate_text_ids)\n",
    "print(generate_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"red\"><b>GPT模型训练与评估</b></font>\n",
    "- 基于已有数据集和模型定义，完成模型训练。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b>完整代码</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1.64 s\n",
      "Wall time: 1.75 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "# 位置编码\n",
    "class PositionEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, max_len: int = 1024, d_model: int = 512):\n",
    "        super().__init__()\n",
    "\n",
    "        # 初始化一个零矩阵 pe，形状为 (seq_len, d_model)，用于存储位置编码\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "\n",
    "        # 生成从 0 到 max_len-1 的整数序列，表示位置索引。将其变为列向量，形状为 (max_lenn, 1)     \n",
    "        position = torch.arange(start=0, end=max_len, step=1).float().unsqueeze(1)\n",
    "\n",
    "        ## 位置编码计算公式：\n",
    "        ## PE(pos, 2i)   = sin(pos / 10000^(2i/d_model))\n",
    "        ## PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))\n",
    "\n",
    "        embedding_index = torch.arange(start=0, end=d_model, step=2).float()\n",
    "\n",
    "        # 计算频率缩放因子\n",
    "        div_term = 1 / torch.tensor(10000.0) ** (embedding_index / d_model)\n",
    "\n",
    "        # 应用正弦和余弦函数计算位置编码\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)  ## every other column, starting with the 1st, has sin() values\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)  ## every other column, starting with the 2nd, has cos() values\n",
    "        pe = pe.unsqueeze(0)  # 在第0维添加一个维度，变成 (1, max_len, d_model)\n",
    "\n",
    "        ## 注册位置编码矩阵\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "        # 词向量叠加位置编码向量\n",
    "    def forward(self, word_embeddings):\n",
    "        return word_embeddings + self.pe[:, :word_embeddings.size(1), :]\n",
    "\n",
    "# 单头注意力计算\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, d_model, head_size, block_size, dropout):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(d_model, head_size, bias=False)\n",
    "        self.query = nn.Linear(d_model, head_size, bias=False)\n",
    "        self.value = nn.Linear(d_model, head_size, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch, seq_len, d_model = x.shape\n",
    "        # 线性变换得到 K 值\n",
    "        K = self.key(x)   # (batch, seq_len, head_size)\n",
    "        # 线性变换得到 Q 值\n",
    "        Q = self.query(x) # (batch, seq_len, head_size)\n",
    "        # 计算注意力得分\n",
    "        # (batch, seq_len, head_size)->(batch, seq_len, seq_len)\n",
    "        wei = Q @ K.transpose(-2,-1) * d_model**-0.5 \n",
    "        # 掩码操作，以确保每个时间步只能看到当前或之前的词汇\n",
    "        # (batch, seq_len, seq_len)\n",
    "        mask = torch.tril(torch.ones((1, seq_len, seq_len))).to(x.device)\n",
    "        wei = wei.masked_fill( mask == 0, float('-inf'))\n",
    "        # 通过 softmax 函数将注意力得分归一化，使得每个查询的注意力得分变成一个概率分布\n",
    "        wei = F.softmax(wei, dim=-1) # (batch, seq_len, seq_len)\n",
    "        wei = self.dropout(wei)\n",
    "        # 线性变换得到 V 值\n",
    "        V = self.value(x) # (batch, seq_len, head_size)\n",
    "        # wei @ v 是对值（Value）的加权求和，得到最终的输出 out，形状为 (batch, seq_len, head_size)\n",
    "        # # (batch, seq_len, seq_len) @ (batch, seq_len, head_size) -> (batch, seq_len, head_size)\n",
    "        out = wei @ V  \n",
    "        return out\n",
    "\n",
    "# 多个注意力头并行计算，然后将其结果合并\n",
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, num_heads, block_size, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        # 通过 ModuleList 创建多个 Head 实例，每个头的大小为 head_size，共有 num_heads 个头\n",
    "        # ModuleList 允许将这些子模块（即多个头）一起管理\n",
    "        head_size = d_model // num_heads\n",
    "        self.heads = nn.ModuleList([Head(d_model, head_size, block_size, dropout) for _ in range(num_heads)])\n",
    "       \n",
    "        # 将多个注意力头的输出拼接起来后映射回原始的嵌入空间\n",
    "        self.proj = nn.Linear(num_heads * head_size, num_heads * head_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 对每个注意力头 h(x) 计算得到的输出进行拼接，拼接的维度是最后一维（dim=-1）。\n",
    "        # 每个头的输出 (batch, seq_len, head_size) \n",
    "        # 会被拼接成一个更大的张量 (batch, seq_len, num_heads * head_size)\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "        \n",
    "# 前馈网络\n",
    "class FeedFoward(nn.Module):\n",
    "    def __init__(self, d_model, dropout):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(d_model, 4 * d_model, name='fc1'),\n",
    "            nn.GELU(),    # GPT3由ReLU替换为GeLU\n",
    "            nn.Linear(4 * d_model, d_model, name='fc2'),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "# 单层解码器\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, block_size, dropout):\n",
    "        # d_model: 词向量长度, num_heads: 注意力头数, block_size:最大文本块长度\n",
    "        super().__init__()\n",
    "        self.ma = MultiHeadAttention(d_model, num_heads, block_size, dropout)\n",
    "        self.ffwd = FeedFoward(d_model, dropout)\n",
    "        self.ln1 = nn.LayerNorm(d_model)  # 层标准化\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.ma(self.ln1(x))  # 此处先标准化，再计算多头注意力\n",
    "        x = x + self.ffwd(self.ln2(x)) # 先层标准化，再前馈网络\n",
    "        return x\n",
    "\n",
    "\n",
    "# GPT类封装\n",
    "class GPT(nn.Module):\n",
    "\n",
    "    def __init__(self, \n",
    "                 vocab_size: int, \n",
    "                 d_model: int = 512, \n",
    "                 n_layer: int = 6, \n",
    "                 n_head: int = 8, \n",
    "                 block_size:int = 16,\n",
    "                 dropout:int = 0.1):\n",
    "        super().__init__()\n",
    "        self.block_size = block_size\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model) # 定义输入层\n",
    "        self.position = PositionEncoding(d_model = d_model)  # 定义位置编码层       \n",
    "        self.blocks = nn.Sequential(*[DecoderBlock(d_model, \n",
    "                                                   num_heads=n_head, \n",
    "                                                   block_size=block_size, dropout = dropout) \\\n",
    "                                      for _ in range(n_layer)])\n",
    "        self.ln = nn.LayerNorm(d_model) # 输出层标准化\n",
    "        self.output = nn.Linear(d_model, vocab_size)  # 定义输出层\n",
    "        # 输出层与词嵌入层共享参数，这和 LLama、通义千问等开源模型类似\n",
    "        # self.embedding.weight = self.output.weight\n",
    "\n",
    "    def forward(self, idx):       \n",
    "        idx = idx.long()  # 将索引转换为 LongTensor\n",
    "        # (batch, seq_len) --> (batch, seq_len, d_model)\n",
    "        x = self.embedding(idx)\n",
    "        x = self.position(x)\n",
    "        x = self.blocks(x)   #  (batch, seq_len, d_model)\n",
    "        x = self.ln(x)   #  (batch, seq_len, d_model)\n",
    "        logits = self.output(x)   # (batch, seq_len,vocab_size)\n",
    "\n",
    "        return logits\n",
    "              \n",
    "    # 模型自回归持续推理，生成长度为 max_new_tokens 的文本块\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx 的维度为： (batch, seq_len) ，表示当前上下文token在词典中的索引（indices）\n",
    "        # max_new_tokens表示新生成的文本的最大长度\n",
    "        for _ in range(max_new_tokens):\n",
    "            # 切片取出每个样本最后 block_size 个 tokens 元素\n",
    "            idx_cond = idx[:, -self.block_size:]\n",
    "            # 模型正向传播\n",
    "            logits = self(idx_cond)  \n",
    "            # 选择每个批次中序列的最后一个时间步\n",
    "             # (batch, seq_len, d_model)->(batch, d_model)\n",
    "            logits = logits[:, -1, :]\n",
    "            # 沿着最后一维进行 softmax 归一化，将原始得分转换为概率分布\n",
    "            probs = F.softmax(logits, dim=-1) # (batch, d_model)\n",
    "            # 根据概率分布probs采样下一个字词的索引\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (batch,1)\n",
    "            if (idx_next == 102):   # 遇到[SEP]结束推理\n",
    "                break\n",
    "            # 追加到已有的字词序列中，将两个张量沿着指定的维度连接起来\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (batch, seq_len+1)\n",
    "            yield idx_next\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型定义和初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "         Embedding-1              [-1, 64, 512]      10,817,536\n",
      "  PositionEncoding-2              [-1, 64, 512]               0\n",
      "         LayerNorm-3              [-1, 64, 512]           1,024\n",
      "            Linear-4               [-1, 64, 64]          32,768\n",
      "            Linear-5               [-1, 64, 64]          32,768\n",
      "           Dropout-6               [-1, 64, 64]               0\n",
      "            Linear-7               [-1, 64, 64]          32,768\n",
      "              Head-8               [-1, 64, 64]               0\n",
      "            Linear-9               [-1, 64, 64]          32,768\n",
      "           Linear-10               [-1, 64, 64]          32,768\n",
      "          Dropout-11               [-1, 64, 64]               0\n",
      "           Linear-12               [-1, 64, 64]          32,768\n",
      "             Head-13               [-1, 64, 64]               0\n",
      "           Linear-14               [-1, 64, 64]          32,768\n",
      "           Linear-15               [-1, 64, 64]          32,768\n",
      "          Dropout-16               [-1, 64, 64]               0\n",
      "           Linear-17               [-1, 64, 64]          32,768\n",
      "             Head-18               [-1, 64, 64]               0\n",
      "           Linear-19               [-1, 64, 64]          32,768\n",
      "           Linear-20               [-1, 64, 64]          32,768\n",
      "          Dropout-21               [-1, 64, 64]               0\n",
      "           Linear-22               [-1, 64, 64]          32,768\n",
      "             Head-23               [-1, 64, 64]               0\n",
      "           Linear-24               [-1, 64, 64]          32,768\n",
      "           Linear-25               [-1, 64, 64]          32,768\n",
      "          Dropout-26               [-1, 64, 64]               0\n",
      "           Linear-27               [-1, 64, 64]          32,768\n",
      "             Head-28               [-1, 64, 64]               0\n",
      "           Linear-29               [-1, 64, 64]          32,768\n",
      "           Linear-30               [-1, 64, 64]          32,768\n",
      "          Dropout-31               [-1, 64, 64]               0\n",
      "           Linear-32               [-1, 64, 64]          32,768\n",
      "             Head-33               [-1, 64, 64]               0\n",
      "           Linear-34               [-1, 64, 64]          32,768\n",
      "           Linear-35               [-1, 64, 64]          32,768\n",
      "          Dropout-36               [-1, 64, 64]               0\n",
      "           Linear-37               [-1, 64, 64]          32,768\n",
      "             Head-38               [-1, 64, 64]               0\n",
      "           Linear-39               [-1, 64, 64]          32,768\n",
      "           Linear-40               [-1, 64, 64]          32,768\n",
      "          Dropout-41               [-1, 64, 64]               0\n",
      "           Linear-42               [-1, 64, 64]          32,768\n",
      "             Head-43               [-1, 64, 64]               0\n",
      "           Linear-44              [-1, 64, 512]         262,656\n",
      "          Dropout-45              [-1, 64, 512]               0\n",
      "MultiHeadAttention-46              [-1, 64, 512]               0\n",
      "        LayerNorm-47              [-1, 64, 512]           1,024\n",
      "           Linear-48             [-1, 64, 2048]       1,050,624\n",
      "             GELU-49             [-1, 64, 2048]               0\n",
      "           Linear-50              [-1, 64, 512]       1,049,088\n",
      "          Dropout-51              [-1, 64, 512]               0\n",
      "       FeedFoward-52              [-1, 64, 512]               0\n",
      "     DecoderBlock-53              [-1, 64, 512]               0\n",
      "        LayerNorm-54              [-1, 64, 512]           1,024\n",
      "           Linear-55               [-1, 64, 64]          32,768\n",
      "           Linear-56               [-1, 64, 64]          32,768\n",
      "          Dropout-57               [-1, 64, 64]               0\n",
      "           Linear-58               [-1, 64, 64]          32,768\n",
      "             Head-59               [-1, 64, 64]               0\n",
      "           Linear-60               [-1, 64, 64]          32,768\n",
      "           Linear-61               [-1, 64, 64]          32,768\n",
      "          Dropout-62               [-1, 64, 64]               0\n",
      "           Linear-63               [-1, 64, 64]          32,768\n",
      "             Head-64               [-1, 64, 64]               0\n",
      "           Linear-65               [-1, 64, 64]          32,768\n",
      "           Linear-66               [-1, 64, 64]          32,768\n",
      "          Dropout-67               [-1, 64, 64]               0\n",
      "           Linear-68               [-1, 64, 64]          32,768\n",
      "             Head-69               [-1, 64, 64]               0\n",
      "           Linear-70               [-1, 64, 64]          32,768\n",
      "           Linear-71               [-1, 64, 64]          32,768\n",
      "          Dropout-72               [-1, 64, 64]               0\n",
      "           Linear-73               [-1, 64, 64]          32,768\n",
      "             Head-74               [-1, 64, 64]               0\n",
      "           Linear-75               [-1, 64, 64]          32,768\n",
      "           Linear-76               [-1, 64, 64]          32,768\n",
      "          Dropout-77               [-1, 64, 64]               0\n",
      "           Linear-78               [-1, 64, 64]          32,768\n",
      "             Head-79               [-1, 64, 64]               0\n",
      "           Linear-80               [-1, 64, 64]          32,768\n",
      "           Linear-81               [-1, 64, 64]          32,768\n",
      "          Dropout-82               [-1, 64, 64]               0\n",
      "           Linear-83               [-1, 64, 64]          32,768\n",
      "             Head-84               [-1, 64, 64]               0\n",
      "           Linear-85               [-1, 64, 64]          32,768\n",
      "           Linear-86               [-1, 64, 64]          32,768\n",
      "          Dropout-87               [-1, 64, 64]               0\n",
      "           Linear-88               [-1, 64, 64]          32,768\n",
      "             Head-89               [-1, 64, 64]               0\n",
      "           Linear-90               [-1, 64, 64]          32,768\n",
      "           Linear-91               [-1, 64, 64]          32,768\n",
      "          Dropout-92               [-1, 64, 64]               0\n",
      "           Linear-93               [-1, 64, 64]          32,768\n",
      "             Head-94               [-1, 64, 64]               0\n",
      "           Linear-95              [-1, 64, 512]         262,656\n",
      "          Dropout-96              [-1, 64, 512]               0\n",
      "MultiHeadAttention-97              [-1, 64, 512]               0\n",
      "        LayerNorm-98              [-1, 64, 512]           1,024\n",
      "           Linear-99             [-1, 64, 2048]       1,050,624\n",
      "            GELU-100             [-1, 64, 2048]               0\n",
      "          Linear-101              [-1, 64, 512]       1,049,088\n",
      "         Dropout-102              [-1, 64, 512]               0\n",
      "      FeedFoward-103              [-1, 64, 512]               0\n",
      "    DecoderBlock-104              [-1, 64, 512]               0\n",
      "       LayerNorm-105              [-1, 64, 512]           1,024\n",
      "          Linear-106               [-1, 64, 64]          32,768\n",
      "          Linear-107               [-1, 64, 64]          32,768\n",
      "         Dropout-108               [-1, 64, 64]               0\n",
      "          Linear-109               [-1, 64, 64]          32,768\n",
      "            Head-110               [-1, 64, 64]               0\n",
      "          Linear-111               [-1, 64, 64]          32,768\n",
      "          Linear-112               [-1, 64, 64]          32,768\n",
      "         Dropout-113               [-1, 64, 64]               0\n",
      "          Linear-114               [-1, 64, 64]          32,768\n",
      "            Head-115               [-1, 64, 64]               0\n",
      "          Linear-116               [-1, 64, 64]          32,768\n",
      "          Linear-117               [-1, 64, 64]          32,768\n",
      "         Dropout-118               [-1, 64, 64]               0\n",
      "          Linear-119               [-1, 64, 64]          32,768\n",
      "            Head-120               [-1, 64, 64]               0\n",
      "          Linear-121               [-1, 64, 64]          32,768\n",
      "          Linear-122               [-1, 64, 64]          32,768\n",
      "         Dropout-123               [-1, 64, 64]               0\n",
      "          Linear-124               [-1, 64, 64]          32,768\n",
      "            Head-125               [-1, 64, 64]               0\n",
      "          Linear-126               [-1, 64, 64]          32,768\n",
      "          Linear-127               [-1, 64, 64]          32,768\n",
      "         Dropout-128               [-1, 64, 64]               0\n",
      "          Linear-129               [-1, 64, 64]          32,768\n",
      "            Head-130               [-1, 64, 64]               0\n",
      "          Linear-131               [-1, 64, 64]          32,768\n",
      "          Linear-132               [-1, 64, 64]          32,768\n",
      "         Dropout-133               [-1, 64, 64]               0\n",
      "          Linear-134               [-1, 64, 64]          32,768\n",
      "            Head-135               [-1, 64, 64]               0\n",
      "          Linear-136               [-1, 64, 64]          32,768\n",
      "          Linear-137               [-1, 64, 64]          32,768\n",
      "         Dropout-138               [-1, 64, 64]               0\n",
      "          Linear-139               [-1, 64, 64]          32,768\n",
      "            Head-140               [-1, 64, 64]               0\n",
      "          Linear-141               [-1, 64, 64]          32,768\n",
      "          Linear-142               [-1, 64, 64]          32,768\n",
      "         Dropout-143               [-1, 64, 64]               0\n",
      "          Linear-144               [-1, 64, 64]          32,768\n",
      "            Head-145               [-1, 64, 64]               0\n",
      "          Linear-146              [-1, 64, 512]         262,656\n",
      "         Dropout-147              [-1, 64, 512]               0\n",
      "MultiHeadAttention-148              [-1, 64, 512]               0\n",
      "       LayerNorm-149              [-1, 64, 512]           1,024\n",
      "          Linear-150             [-1, 64, 2048]       1,050,624\n",
      "            GELU-151             [-1, 64, 2048]               0\n",
      "          Linear-152              [-1, 64, 512]       1,049,088\n",
      "         Dropout-153              [-1, 64, 512]               0\n",
      "      FeedFoward-154              [-1, 64, 512]               0\n",
      "    DecoderBlock-155              [-1, 64, 512]               0\n",
      "       LayerNorm-156              [-1, 64, 512]           1,024\n",
      "          Linear-157               [-1, 64, 64]          32,768\n",
      "          Linear-158               [-1, 64, 64]          32,768\n",
      "         Dropout-159               [-1, 64, 64]               0\n",
      "          Linear-160               [-1, 64, 64]          32,768\n",
      "            Head-161               [-1, 64, 64]               0\n",
      "          Linear-162               [-1, 64, 64]          32,768\n",
      "          Linear-163               [-1, 64, 64]          32,768\n",
      "         Dropout-164               [-1, 64, 64]               0\n",
      "          Linear-165               [-1, 64, 64]          32,768\n",
      "            Head-166               [-1, 64, 64]               0\n",
      "          Linear-167               [-1, 64, 64]          32,768\n",
      "          Linear-168               [-1, 64, 64]          32,768\n",
      "         Dropout-169               [-1, 64, 64]               0\n",
      "          Linear-170               [-1, 64, 64]          32,768\n",
      "            Head-171               [-1, 64, 64]               0\n",
      "          Linear-172               [-1, 64, 64]          32,768\n",
      "          Linear-173               [-1, 64, 64]          32,768\n",
      "         Dropout-174               [-1, 64, 64]               0\n",
      "          Linear-175               [-1, 64, 64]          32,768\n",
      "            Head-176               [-1, 64, 64]               0\n",
      "          Linear-177               [-1, 64, 64]          32,768\n",
      "          Linear-178               [-1, 64, 64]          32,768\n",
      "         Dropout-179               [-1, 64, 64]               0\n",
      "          Linear-180               [-1, 64, 64]          32,768\n",
      "            Head-181               [-1, 64, 64]               0\n",
      "          Linear-182               [-1, 64, 64]          32,768\n",
      "          Linear-183               [-1, 64, 64]          32,768\n",
      "         Dropout-184               [-1, 64, 64]               0\n",
      "          Linear-185               [-1, 64, 64]          32,768\n",
      "            Head-186               [-1, 64, 64]               0\n",
      "          Linear-187               [-1, 64, 64]          32,768\n",
      "          Linear-188               [-1, 64, 64]          32,768\n",
      "         Dropout-189               [-1, 64, 64]               0\n",
      "          Linear-190               [-1, 64, 64]          32,768\n",
      "            Head-191               [-1, 64, 64]               0\n",
      "          Linear-192               [-1, 64, 64]          32,768\n",
      "          Linear-193               [-1, 64, 64]          32,768\n",
      "         Dropout-194               [-1, 64, 64]               0\n",
      "          Linear-195               [-1, 64, 64]          32,768\n",
      "            Head-196               [-1, 64, 64]               0\n",
      "          Linear-197              [-1, 64, 512]         262,656\n",
      "         Dropout-198              [-1, 64, 512]               0\n",
      "MultiHeadAttention-199              [-1, 64, 512]               0\n",
      "       LayerNorm-200              [-1, 64, 512]           1,024\n",
      "          Linear-201             [-1, 64, 2048]       1,050,624\n",
      "            GELU-202             [-1, 64, 2048]               0\n",
      "          Linear-203              [-1, 64, 512]       1,049,088\n",
      "         Dropout-204              [-1, 64, 512]               0\n",
      "      FeedFoward-205              [-1, 64, 512]               0\n",
      "    DecoderBlock-206              [-1, 64, 512]               0\n",
      "       LayerNorm-207              [-1, 64, 512]           1,024\n",
      "          Linear-208               [-1, 64, 64]          32,768\n",
      "          Linear-209               [-1, 64, 64]          32,768\n",
      "         Dropout-210               [-1, 64, 64]               0\n",
      "          Linear-211               [-1, 64, 64]          32,768\n",
      "            Head-212               [-1, 64, 64]               0\n",
      "          Linear-213               [-1, 64, 64]          32,768\n",
      "          Linear-214               [-1, 64, 64]          32,768\n",
      "         Dropout-215               [-1, 64, 64]               0\n",
      "          Linear-216               [-1, 64, 64]          32,768\n",
      "            Head-217               [-1, 64, 64]               0\n",
      "          Linear-218               [-1, 64, 64]          32,768\n",
      "          Linear-219               [-1, 64, 64]          32,768\n",
      "         Dropout-220               [-1, 64, 64]               0\n",
      "          Linear-221               [-1, 64, 64]          32,768\n",
      "            Head-222               [-1, 64, 64]               0\n",
      "          Linear-223               [-1, 64, 64]          32,768\n",
      "          Linear-224               [-1, 64, 64]          32,768\n",
      "         Dropout-225               [-1, 64, 64]               0\n",
      "          Linear-226               [-1, 64, 64]          32,768\n",
      "            Head-227               [-1, 64, 64]               0\n",
      "          Linear-228               [-1, 64, 64]          32,768\n",
      "          Linear-229               [-1, 64, 64]          32,768\n",
      "         Dropout-230               [-1, 64, 64]               0\n",
      "          Linear-231               [-1, 64, 64]          32,768\n",
      "            Head-232               [-1, 64, 64]               0\n",
      "          Linear-233               [-1, 64, 64]          32,768\n",
      "          Linear-234               [-1, 64, 64]          32,768\n",
      "         Dropout-235               [-1, 64, 64]               0\n",
      "          Linear-236               [-1, 64, 64]          32,768\n",
      "            Head-237               [-1, 64, 64]               0\n",
      "          Linear-238               [-1, 64, 64]          32,768\n",
      "          Linear-239               [-1, 64, 64]          32,768\n",
      "         Dropout-240               [-1, 64, 64]               0\n",
      "          Linear-241               [-1, 64, 64]          32,768\n",
      "            Head-242               [-1, 64, 64]               0\n",
      "          Linear-243               [-1, 64, 64]          32,768\n",
      "          Linear-244               [-1, 64, 64]          32,768\n",
      "         Dropout-245               [-1, 64, 64]               0\n",
      "          Linear-246               [-1, 64, 64]          32,768\n",
      "            Head-247               [-1, 64, 64]               0\n",
      "          Linear-248              [-1, 64, 512]         262,656\n",
      "         Dropout-249              [-1, 64, 512]               0\n",
      "MultiHeadAttention-250              [-1, 64, 512]               0\n",
      "       LayerNorm-251              [-1, 64, 512]           1,024\n",
      "          Linear-252             [-1, 64, 2048]       1,050,624\n",
      "            GELU-253             [-1, 64, 2048]               0\n",
      "          Linear-254              [-1, 64, 512]       1,049,088\n",
      "         Dropout-255              [-1, 64, 512]               0\n",
      "      FeedFoward-256              [-1, 64, 512]               0\n",
      "    DecoderBlock-257              [-1, 64, 512]               0\n",
      "       LayerNorm-258              [-1, 64, 512]           1,024\n",
      "          Linear-259               [-1, 64, 64]          32,768\n",
      "          Linear-260               [-1, 64, 64]          32,768\n",
      "         Dropout-261               [-1, 64, 64]               0\n",
      "          Linear-262               [-1, 64, 64]          32,768\n",
      "            Head-263               [-1, 64, 64]               0\n",
      "          Linear-264               [-1, 64, 64]          32,768\n",
      "          Linear-265               [-1, 64, 64]          32,768\n",
      "         Dropout-266               [-1, 64, 64]               0\n",
      "          Linear-267               [-1, 64, 64]          32,768\n",
      "            Head-268               [-1, 64, 64]               0\n",
      "          Linear-269               [-1, 64, 64]          32,768\n",
      "          Linear-270               [-1, 64, 64]          32,768\n",
      "         Dropout-271               [-1, 64, 64]               0\n",
      "          Linear-272               [-1, 64, 64]          32,768\n",
      "            Head-273               [-1, 64, 64]               0\n",
      "          Linear-274               [-1, 64, 64]          32,768\n",
      "          Linear-275               [-1, 64, 64]          32,768\n",
      "         Dropout-276               [-1, 64, 64]               0\n",
      "          Linear-277               [-1, 64, 64]          32,768\n",
      "            Head-278               [-1, 64, 64]               0\n",
      "          Linear-279               [-1, 64, 64]          32,768\n",
      "          Linear-280               [-1, 64, 64]          32,768\n",
      "         Dropout-281               [-1, 64, 64]               0\n",
      "          Linear-282               [-1, 64, 64]          32,768\n",
      "            Head-283               [-1, 64, 64]               0\n",
      "          Linear-284               [-1, 64, 64]          32,768\n",
      "          Linear-285               [-1, 64, 64]          32,768\n",
      "         Dropout-286               [-1, 64, 64]               0\n",
      "          Linear-287               [-1, 64, 64]          32,768\n",
      "            Head-288               [-1, 64, 64]               0\n",
      "          Linear-289               [-1, 64, 64]          32,768\n",
      "          Linear-290               [-1, 64, 64]          32,768\n",
      "         Dropout-291               [-1, 64, 64]               0\n",
      "          Linear-292               [-1, 64, 64]          32,768\n",
      "            Head-293               [-1, 64, 64]               0\n",
      "          Linear-294               [-1, 64, 64]          32,768\n",
      "          Linear-295               [-1, 64, 64]          32,768\n",
      "         Dropout-296               [-1, 64, 64]               0\n",
      "          Linear-297               [-1, 64, 64]          32,768\n",
      "            Head-298               [-1, 64, 64]               0\n",
      "          Linear-299              [-1, 64, 512]         262,656\n",
      "         Dropout-300              [-1, 64, 512]               0\n",
      "MultiHeadAttention-301              [-1, 64, 512]               0\n",
      "       LayerNorm-302              [-1, 64, 512]           1,024\n",
      "          Linear-303             [-1, 64, 2048]       1,050,624\n",
      "            GELU-304             [-1, 64, 2048]               0\n",
      "          Linear-305              [-1, 64, 512]       1,049,088\n",
      "         Dropout-306              [-1, 64, 512]               0\n",
      "      FeedFoward-307              [-1, 64, 512]               0\n",
      "    DecoderBlock-308              [-1, 64, 512]               0\n",
      "       LayerNorm-309              [-1, 64, 512]           1,024\n",
      "          Linear-310            [-1, 64, 21128]      10,838,664\n",
      "================================================================\n",
      "Total params: 40,562,312\n",
      "Trainable params: 40,562,312\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 44.07\n",
      "Params size (MB): 154.73\n",
      "Estimated Total Size (MB): 198.80\n",
      "----------------------------------------------------------------\n",
      "None\n",
      "40.562312 M parameters\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from model import GPT\n",
    "# 参数设置\n",
    "random_seed = 2025    # 随机数种子\n",
    "torch.manual_seed(random_seed)\n",
    "vocab_size = 21128   # 词典大小\n",
    "batch_size = 64\n",
    "block_size = 16     # 模型最长文本推理能力\n",
    "total_train_steps = 2000  # 训练步数\n",
    "eval_interval = 200  # 训练集评估迭代步数\n",
    "learning_rate = 1e-4\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200  # 验证集评估迭代步数\n",
    "d_model = 512\n",
    "n_head = 8\n",
    "n_layer = 6\n",
    "dropout = 0.1\n",
    "# ------------\n",
    "\n",
    "model = GPT(vocab_size, \n",
    "            d_model, \n",
    "            n_layer=n_layer, \n",
    "            n_head=n_head, \n",
    "            block_size=block_size,\n",
    "            dropout = dropout)\n",
    "# 初始化模型参数，对于维度大于 1 的参数，使用 Xavier 均匀初始化方法进行初始化\n",
    "for p in model.parameters():  # 遍历模型中的可学习参数 \n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# 优化算法\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "from torchsummary import summary\n",
    "\n",
    "print(summary(model, (64,)))  # 输入形状应该是 (block_size, ),即文本的最大长度 block_size\n",
    "# 打印模型参数\n",
    "print(sum(p.numel() for p in model.parameters()) / 1e6, 'M parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "准备训练数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "批次 1:\n",
      "输入序列： tensor([[ 702, 8024,  821,    0,    0,    0,    0,    0],\n",
      "        [ 511, 1762,    0,    0,    0,    0,    0,    0],\n",
      "        [5790, 1310, 4495, 4518,    0,    0,    0,    0],\n",
      "        [8024,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [1355,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [ 868, 3322, 1169, 8024, 2828,  100,    0,    0],\n",
      "        [7481,  707, 1392, 4905, 1377,    0,    0,    0],\n",
      "        [4495,  772, 1555, 1469, 3867, 6589, 5442,  722],\n",
      "        [7188, 1154,    0,    0,    0,    0,    0,    0],\n",
      "        [ 916, 6624,  677, 1158,  689,  722, 6662, 8024],\n",
      "        [ 671, 3797,  831, 4899,  837,    0,    0,    0],\n",
      "        [4802, 2900, 1139, 8024,  100, 1062,    0,    0],\n",
      "        [1762, 6821, 7027, 8024, 2769, 1403,    0,    0],\n",
      "        [7357, 2157, 1804, 3333,    0,    0,    0,    0],\n",
      "        [ 100, 1993,    0,    0,    0,    0,    0,    0],\n",
      "        [6084, 3118, 2898,  511, 1041,    0,    0,    0],\n",
      "        [1265, 1355,    0,    0,    0,    0,    0,    0],\n",
      "        [3191, 6956,  816,    0,    0,    0,    0,    0],\n",
      "        [6662,  100,  956, 6379, 8024,    0,    0,    0],\n",
      "        [4989, 3428, 5143, 5320, 8024, 1780, 1104,    0],\n",
      "        [1277, 6392, 3177, 6809, 1168,  679, 1217,  818],\n",
      "        [2466, 2990,    0,    0,    0,    0,    0,    0],\n",
      "        [7325, 6438, 3175, 2466, 8024, 2802,    0,    0],\n",
      "        [3634, 4638, 3946, 3265,  511,  100,  794, 1184],\n",
      "        [2397, 1139, 3173,    0,    0,    0,    0,    0],\n",
      "        [ 100,  100, 4263,    0,    0,    0,    0,    0],\n",
      "        [3341, 8024, 2769,    0,    0,    0,    0,    0],\n",
      "        [1398, 2146,    0,    0,    0,    0,    0,    0],\n",
      "        [4415, 2356, 7213, 3441, 7252,    0,    0,    0],\n",
      "        [3322,  677, 4638, 2429,    0,    0,    0,    0],\n",
      "        [5509,  704, 3744, 1357, 1724, 4905, 3255,    0],\n",
      "        [6821, 4905,    0,    0,    0,    0,    0,    0],\n",
      "        [2424, 4664, 5052, 5543, 1213,    0,    0,    0],\n",
      "        [ 511, 1400, 3341, 8024,  753,  782, 5310,  749],\n",
      "        [1358, 1168, 4385, 1767, 6225,  830, 3614,    0],\n",
      "        [6598, 3975, 2832, 1057, 5310, 3354,    0,    0],\n",
      "        [1744,  831, 4899, 1344, 1999,    0,    0,    0],\n",
      "        [1218, 7368,    0,    0,    0,    0,    0,    0],\n",
      "        [1168, 2571, 6862,    0,    0,    0,    0,    0],\n",
      "        [8024, 2512, 1510, 4708,  800,  812,    0,    0],\n",
      "        [2769, 1744, 2129, 1039, 3198, 3309, 3862,    0],\n",
      "        [ 800, 6819,  740, 1158,  689, 8024, 7390,    0],\n",
      "        [8024, 7357, 4324, 4899, 6206,    0,    0,    0],\n",
      "        [1737, 1765, 1277, 1469, 6577, 1737,    0,    0],\n",
      "        [7357, 1069, 3360, 6432,    0,    0,    0,    0],\n",
      "        [2773, 8203, 1453, 2399,    0,    0,    0,    0],\n",
      "        [3173, 7770, 1765,  510, 2972, 1220, 1914, 6804],\n",
      "        [2399, 8024, 1305, 2861, 5838,    0,    0,    0],\n",
      "        [2398, 2600,  741, 6381, 1384,    0,    0,    0],\n",
      "        [3124, 2424, 6956, 7305,    0,    0,    0,    0],\n",
      "        [2571, 8024, 6820, 3221,    0,    0,    0,    0],\n",
      "        [4706, 3765, 3333,    0,    0,    0,    0,    0],\n",
      "        [3255, 7310, 1068, 3952, 2767,  100,  756,    0],\n",
      "        [1071,  704, 1290, 3779, 9083, 8148,  510,    0],\n",
      "        [1780,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [3777, 1298, 3821, 7345,    0,    0,    0,    0],\n",
      "        [7361,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [ 671,  763, 7770, 3413, 6820, 2768, 4989,  749],\n",
      "        [2193, 8024,    0,    0,    0,    0,    0,    0],\n",
      "        [1762, 6821,  671, 6814, 4923,  704, 8024, 3173],\n",
      "        [3184, 8021,    0,    0,    0,    0,    0,    0],\n",
      "        [ 752,  837, 5320, 2797, 2339,  689,  510,    0],\n",
      "        [3813,  691, 2792,    0,    0,    0,    0,    0],\n",
      "        [8439, 2399, 8024, 1313,    0,    0,    0,    0]])\n",
      "标签： tensor([ 689,  704,  510, 2376, 2245,  697,  809, 7313, 6756, 6820, 5320, 3300,\n",
      "         686, 7728, 5303, 1146, 2245, 8024, 3192, 7344,  862, 1139, 6858, 2791,\n",
      "        1921, 1744,  812, 2357, 7911, 3488, 2716, 4777, 8024, 2042, 6816, 8024,\n",
      "         741, 1355, 2768, 3297,  677, 1400, 3724, 5408, 8038, 5023, 1469, 1164,\n",
      "        1374, 3124,  833, 2456, 3952, 1290, 2898, 2356, 3198, 3413, 4989, 1798,\n",
      "        1298, 3180, 6432, 1355])\n",
      "\n",
      "批次 2:\n",
      "输入序列： tensor([[5328,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [ 100, 8024, 6421, 6566, 6569, 1398,    0,    0],\n",
      "        [1762, 4448, 7032,    0,    0,    0,    0,    0],\n",
      "        [ 100, 5356, 5442, 7509,  727,  794,  677,    0],\n",
      "        [1184, 4638, 8467,  119,  128,  677, 1285,    0],\n",
      "        [4495, 6778, 8024, 2400,  684, 6158, 6948,    0],\n",
      "        [1351,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [ 678, 8024, 3177,  671, 1469, 1398,  752,    0],\n",
      "        [7270, 1076, 2820, 6577, 1215,  712,  818,    0],\n",
      "        [4275, 8024,    0,    0,    0,    0,    0,    0],\n",
      "        [6392,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [2476, 8024,  677,  837, 4212, 4275, 8208, 8599],\n",
      "        [4617,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [7674, 6206, 3738, 3381, 4289, 2600, 4840,    0],\n",
      "        [3717,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [4918, 4902, 2231, 6208, 4667,    0,    0,    0],\n",
      "        [6235,  671,  860, 1265, 7032, 6084,    0,    0],\n",
      "        [4638, 2160, 2408,    0,    0,    0,    0,    0],\n",
      "        [1232, 8024,  679, 2418, 6428, 6438,    0,    0],\n",
      "        [3118, 2898, 1914, 6804,    0,    0,    0,    0],\n",
      "        [3341, 2342,    0,    0,    0,    0,    0,    0],\n",
      "        [2157, 2128, 1059, 8024, 3362,    0,    0,    0],\n",
      "        [2682,  976,  671,    0,    0,    0,    0,    0],\n",
      "        [ 679,  852, 5543, 3300,    0,    0,    0,    0],\n",
      "        [1072, 1906, 2495, 2400, 3340,  816, 4638, 4178],\n",
      "        [8020, 3173, 1290, 4852, 1266,  776,  127,    0],\n",
      "        [2511, 3227,  749,  704, 1744, 1066,  772, 1054],\n",
      "        [1925, 1092, 1999, 3124, 3780, 2339,  868,    0],\n",
      "        [1282,  676, 2237, 1059, 1744,  782, 1920,    0],\n",
      "        [4197, 7476, 5905, 2339, 5686, 1814,  100, 8024],\n",
      "        [3833, 1750,  100, 2357, 2229, 8024,    0,    0],\n",
      "        [1894, 4638,    0,    0,    0,    0,    0,    0],\n",
      "        [ 110, 2990, 1285, 5635,    0,    0,    0,    0],\n",
      "        [5299,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [ 704, 1925,    0,    0,    0,    0,    0,    0],\n",
      "        [2825, 3717,    0,    0,    0,    0,    0,    0],\n",
      "        [5307, 3845, 1355, 2245,  704, 4638,    0,    0],\n",
      "        [3362,  510, 4852,  833, 3126, 3362, 4638,    0],\n",
      "        [ 511, 6421, 7555, 4680, 2199, 6226,    0,    0],\n",
      "        [ 738, 3766, 3160, 1168, 5632, 2346,  833,    0],\n",
      "        [4495, 3833, 6574, 7030,  511,    0,    0,    0],\n",
      "        [2471,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [1238,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [6225,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [3146,  702, 7942, 3777, 7674,    0,    0,    0],\n",
      "        [1146, 4708, 2593,  511,  128, 3299, 8162,    0],\n",
      "        [2094,  511,  100, 5636, 1213, 4495,    0,    0],\n",
      "        [3419, 5384,  762, 6432,    0,    0,    0,    0],\n",
      "        [8024, 2141, 4385, 5299, 5302,    0,    0,    0],\n",
      "        [4263, 1744, 5442,    0,    0,    0,    0,    0],\n",
      "        [ 833,  510,    0,    0,    0,    0,    0,    0],\n",
      "        [2110, 7368, 2408, 2128,    0,    0,    0,    0],\n",
      "        [2552, 6656, 1054, 6624,  510,    0,    0,    0],\n",
      "        [1453, 3309,  924, 7397,  782, 3696,    0,    0],\n",
      "        [ 794,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [1962, 6124, 2970,  511, 4680, 1184, 3309, 6573],\n",
      "        [2957,  671, 6956,    0,    0,    0,    0,    0],\n",
      "        [1744, 3918, 1265, 1394,    0,    0,    0,    0],\n",
      "        [1068, 1355, 2245,  704, 5401, 1068,    0,    0],\n",
      "        [4680, 5023, 4684,    0,    0,    0,    0,    0],\n",
      "        [2773, 8024, 1780, 2137,  679, 4919, 2972, 1220],\n",
      "        [ 100, 5401,    0,    0,    0,    0,    0,    0],\n",
      "        [ 510, 1555,  689,  510, 2797, 2339,    0,    0],\n",
      "        [6399, 7309, 7579, 4685, 5310,    0,    0,    0]])\n",
      "标签： tensor([6963, 2562, 1374, 3862, 5635, 7028, 1962, 6813, 1155, 2372,  511, 2476,\n",
      "        4638, 2399, 4905, 2372, 3302, 5541,  671, 6588, 1920, 3171, 6777, 3126,\n",
      "        5296, 3299, 1993, 6956,  676, 6821, 3291, 3486, 8439, 6205, 6206, 2398,\n",
      "        2372, 5320, 1153, 1963,  100, 3291, 2942, 4692, 3289, 3189, 2578, 8024,\n",
      "        1066, 3780,  704, 7305,  736,  978,  860, 2356, 1146,  868, 5143, 2970,\n",
      "        7770,  714,  689, 1394])\n",
      "\n",
      "批次 3:\n",
      "输入序列： tensor([[ 2157,   727,  6566,  6569,   782,  5314,     0,     0],\n",
      "        [ 4500,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [ 8024,  2844,  7881,  7339,  1126,   725,     0,     0],\n",
      "        [ 7728,  3333,     0,     0,     0,     0,     0,     0],\n",
      "        [ 7566,  7599,  2213,     0,     0,     0,     0,     0],\n",
      "        [ 7027,  4638,  3127,   782,     0,     0,     0,     0],\n",
      "        [ 7029,  7883,     0,     0,     0,     0,     0,     0],\n",
      "        [ 2339,   868,  7339,  7339,  7270,  1076,  5018,     0],\n",
      "        [ 1469,  6450,  1355,  2245,     0,     0,     0,     0],\n",
      "        [ 1762,  3777,     0,     0,     0,     0,     0,     0],\n",
      "        [ 1744,  1079,  1744,  7354,   697,   702,  2356,     0],\n",
      "        [ 5299,  1469,  3249,  6858,  1730,   860,     0,     0],\n",
      "        [  749,  1377,     0,     0,     0,     0,     0,     0],\n",
      "        [ 4263,  1744,   712,   721,  3136,  5509,     0,     0],\n",
      "        [ 3189,  2094,   511,  3341,  5632,   757,  1221,  1344],\n",
      "        [  671,  6956,  1146,  6158,  6163,     0,     0,     0],\n",
      "        [ 1398,  7478,  3175,   671,  6887,  2141,  3177,  1962],\n",
      "        [ 3333,  8024,  5381,  3419,  1447,  3330,  5494,     0],\n",
      "        [ 6206,  2199,   683,   689,  1112,  1265,   831,     0],\n",
      "        [  749,  8024,   738,  3187,  3791,  2533,     0,     0],\n",
      "        [ 6134,  4028,  6820,  3221,  2767,     0,     0,     0],\n",
      "        [  674,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [ 1355,  4638,     0,     0,     0,     0,     0,     0],\n",
      "        [  677,   757,  5468,  5381,  2571,  6756,  8024,     0],\n",
      "        [ 5543,  1355,     0,     0,     0,     0,     0,     0],\n",
      "        [ 5439,  4374,  1400,  4638,  7377,  2422,     0,     0],\n",
      "        [  671,  6581,  6134,     0,     0,     0,     0,     0],\n",
      "        [ 3341,  8024,     0,     0,     0,     0,     0,     0],\n",
      "        [ 1277,  2111,  2094,  2970,     0,     0,     0,     0],\n",
      "        [ 1324,  3771,  3975,   875,  3184,  5632,  3780,  1344],\n",
      "        [  836,  1920,     0,     0,     0,     0,     0,     0],\n",
      "        [  801,  8024,  2376,  1221,     0,     0,     0,     0],\n",
      "        [ 3217,  4495,  1184,   711,  2255,  1298,  2356,     0],\n",
      "        [ 8024,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [ 1736,  8024,  1762,  4663,  4822,     0,     0,     0],\n",
      "        [ 1282,  1126,  1366,     0,     0,     0,     0,     0],\n",
      "        [ 6814,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [ 2376,  1221,  8024,  1086,   738,   679,  4500,   711],\n",
      "        [ 2487,  7608,  1501,  2128,     0,     0,     0,     0],\n",
      "        [ 3198,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  100,  5010,  1677,     0,     0,     0,     0,     0],\n",
      "        [ 7030,  3126,  4660,  8024,   679,     0,     0,     0],\n",
      "        [ 3833,  1220,  4638,  2658,  1105,  8024,     0,     0],\n",
      "        [ 1447,  8024,   800,     0,     0,     0,     0,     0],\n",
      "        [ 3294,  6158,  2852,   677,  1343,     0,     0,     0],\n",
      "        [ 2428,  1121,  2208,   782,  5408,  2642,  4567,     0],\n",
      "        [ 3333,  1062,  1066,  3302,     0,     0,     0,     0],\n",
      "        [ 4906,  1447,  7504,     0,     0,     0,     0,     0],\n",
      "        [ 3490,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  720,  6825,  5330,   715,  1215,  8110,  2399,  4638],\n",
      "        [ 2769,  1744,   691,  1266,  1073,  1798,     0,     0],\n",
      "        [ 3736,  8024,  2769,  7752,  4197,     0,     0,     0],\n",
      "        [ 1138,  7379,  1905,  8024,   981,  2209,  7463,  1139],\n",
      "        [ 2356,  1767,     0,     0,     0,     0,     0,     0],\n",
      "        [ 2768,  4989,   511,  8108,  3299,  8149,  3189,     0],\n",
      "        [ 1759,  1765,  8024,   738,  2199,     0,     0,     0],\n",
      "        [  722,  1333,  1156,   100,     0,     0,     0,     0],\n",
      "        [ 2399,  1456,   511,     0,     0,     0,     0,     0],\n",
      "        [ 2595,  2456,  5029,   511,  2972,  6822,     0,     0],\n",
      "        [ 7030,  1469,  3126,  4372,  8024,   852,  1762,  3364],\n",
      "        [10550,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [ 2399,  4638,  7946,  1759,  1765,   924,  2844,  4777],\n",
      "        [  510,   712,     0,     0,     0,     0,     0,     0],\n",
      "        [ 1462,  6822,  4923,   704,     0,     0,     0,     0]])\n",
      "标签： tensor([3333, 1962, 3680, 2339, 8024,  511, 1469,  671,  791, 1298, 1767, 5299,\n",
      "         809, 4850, 4638, 1762,  704, 1762, 1232, 1168, 1196,  185, 4495,  679,\n",
      "        2916,  511, 4385, 4706, 1358, 1999,  752, 2397, 2820, 5468, 1765, 7824,\n",
      "        4923, 2823, 1059, 1291, 1677, 3171, 2990,  812, 4403,  511, 1218, 1046,\n",
      "        6158,  100, 7946, 2697,  671, 6121,  704, 5632,  100,  100,  740, 5294,\n",
      "        2399, 4955,  818,  671])\n",
      "\n",
      "批次 4:\n",
      "输入序列： tensor([[6392,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [1905, 1343,  749,  511, 6814, 1343,  671, 1168],\n",
      "        [2094,  511,  100,  809, 5125,    0,    0,    0],\n",
      "        [3696, 3791, 1073,    0,    0,    0,    0,    0],\n",
      "        [5632, 4415, 5543,    0,    0,    0,    0,    0],\n",
      "        [1400, 4638,  517, 4495, 4343,    0,    0,    0],\n",
      "        [5862, 2141,  739,    0,    0,    0,    0,    0],\n",
      "        [2971, 3309,    0,    0,    0,    0,    0,    0],\n",
      "        [1170, 3173, 3867, 6589,  860,    0,    0,    0],\n",
      "        [1054, 4638,    0,    0,    0,    0,    0,    0],\n",
      "        [3144, 2110,  510,    0,    0,    0,    0,    0],\n",
      "        [6121, 4638,    0,    0,    0,    0,    0,    0],\n",
      "        [1587,  510, 3696, 2168, 3333, 2487, 8024, 4680],\n",
      "        [3322,  833,  738, 6632, 3341, 6632, 1914,    0],\n",
      "        [1199,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [6756,  100,  511,  772, 1501, 2972, 1139, 1400],\n",
      "        [1380, 1777, 3403, 5143,  704, 2245,    0,    0],\n",
      "        [4696, 3633, 4638, 1914, 6804,  712,  721, 8024],\n",
      "        [6422, 2412, 3633, 1762,  711,    0,    0,    0],\n",
      "        [8024,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [ 754,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [6632, 1914, 8024, 2769,  812,    0,    0,    0],\n",
      "        [ 809,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [1355, 2245, 7348,    0,    0,    0,    0,    0],\n",
      "        [8125, 2207,    0,    0,    0,    0,    0,    0],\n",
      "        [2769, 1744, 1912, 6588,    0,    0,    0,    0],\n",
      "        [8024, 6929, 7027, 6887, 6662,    0,    0,    0],\n",
      "        [1818, 4495, 2578,  924, 2844, 1469,    0,    0],\n",
      "        [6121,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [2458, 6814, 1217, 2339, 1773, 8024, 1400, 1726],\n",
      "        [ 783, 1039, 8024,  782,    0,    0,    0,    0],\n",
      "        [4905, 3175, 2466, 2376, 1221,    0,    0,    0],\n",
      "        [6422, 4638, 1353, 5576, 6292, 6241,  511,    0],\n",
      "        [3696, 1762,    0,    0,    0,    0,    0,    0],\n",
      "        [1767, 4638, 1093,  689,    0,    0,    0,    0],\n",
      "        [2773, 4526, 2137, 1213,    0,    0,    0,    0],\n",
      "        [6574, 4638,  897, 5314, 4080, 1355, 3200, 4670],\n",
      "        [1282, 3613, 7415,  860, 2110,    0,    0,    0],\n",
      "        [2458,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [3724, 8024, 1217,    0,    0,    0,    0,    0],\n",
      "        [4142, 4554, 2658,  510,  914, 6822,    0,    0],\n",
      "        [4916, 1355,    0,    0,    0,    0,    0,    0],\n",
      "        [8021, 2128, 2412, 2356, 1999,    0,    0,    0],\n",
      "        [2595, 4638, 1920, 2110,  511,    0,    0,    0],\n",
      "        [ 782, 3152, 2372, 3341, 4638, 1352, 7028,    0],\n",
      "        [3975, 2128, 1059, 3173, 2773, 4526, 8024, 1780],\n",
      "        [4495, 3833, 2458, 2245,    0,    0,    0,    0],\n",
      "        [4638, 6956, 5392, 2590, 5991, 2347,    0,    0],\n",
      "        [2127, 2266,  855, 8024, 4959, 6121, 1762, 1814],\n",
      "        [8024, 3766,    0,    0,    0,    0,    0,    0],\n",
      "        [ 510, 1453, 7188, 1093,  510,    0,    0,    0],\n",
      "        [7028, 6206,  868, 4500,  511,  100,  100,    0],\n",
      "        [6577,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [3466, 1139,  100,  511,  852, 3466, 3389, 5299],\n",
      "        [6825, 7360, 1920, 7434, 8024,    0,    0,    0],\n",
      "        [2792, 2569,  511, 1762, 6821, 4905,    0,    0],\n",
      "        [2207, 3198, 8024, 1059, 5299, 8133, 2787,    0],\n",
      "        [3300, 1093, 3333, 4638, 2207, 2434,    0,    0],\n",
      "        [1469, 1730, 5310,    0,    0,    0,    0,    0],\n",
      "        [1184, 3341, 4750,    0,    0,    0,    0,    0],\n",
      "        [1044, 4638, 3918, 2428, 2110,  739, 3255,    0],\n",
      "        [6821, 3667, 6413, 6375,    0,    0,    0,    0],\n",
      "        [ 671, 3340,  924, 7397, 4905,    0,    0,    0],\n",
      "        [3632, 4506, 7313, 4916, 3717,    0,    0,    0]])\n",
      "标签： tensor([ 511, 7433, 1501, 3221, 1213, 2248, 6818, 7313, 7741, 1213, 1765, 1213,\n",
      "        1184,  511,  712, 8024, 4385,  794, 1346, 6822, 1059, 2218,  739, 3667,\n",
      "        3198, 6822, 5680, 7770, 4852,  740, 1772, 6577, 6821, 2157, 1825, 2769,\n",
      "        4638,  739, 2245, 2487,  686, 2245, 1199,  100, 5401, 2898, 3152,  719,\n",
      "        2356, 3300, 5882, 2972, 2339,  738, 7506, 2487, 5408, 4294,  511,  814,\n",
      "        5543, 7357, 2094,  511])\n",
      "\n",
      "批次 5:\n",
      "输入序列： tensor([[1469, 2506, 4487, 8024,  684, 7471, 5709,    0],\n",
      "        [4904,  836,  689, 2519, 6854, 1963, 6004,  511],\n",
      "        [3717,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [1818, 4852,    0,    0,    0,    0,    0,    0],\n",
      "        [1107,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [6649,  511,  100, 1322, 2791,    0,    0,    0],\n",
      "        [4080, 1225,    0,    0,    0,    0,    0,    0],\n",
      "        [ 510, 4374,  691, 3209,    0,    0,    0,    0],\n",
      "        [2769, 1744, 4906, 2825,  752,  689, 1357, 2533],\n",
      "        [2458, 7654, 3160, 2128, 1059, 4495,  772,    0],\n",
      "        [ 722, 4883, 2476,    0,    0,    0,    0,    0],\n",
      "        [2573, 3136, 5509,    0,    0,    0,    0,    0],\n",
      "        [4955,  833, 1199,  833, 7270,    0,    0,    0],\n",
      "        [5442, 4522,    0,    0,    0,    0,    0,    0],\n",
      "        [1385, 1054, 5408,    0,    0,    0,    0,    0],\n",
      "        [2787, 6963, 4761, 6887,    0,    0,    0,    0],\n",
      "        [1744,  510, 3123, 4706,    0,    0,    0,    0],\n",
      "        [4638, 5632,    0,    0,    0,    0,    0,    0],\n",
      "        [1367, 1073, 6408, 6404, 8024,    0,    0,    0],\n",
      "        [5442, 2990,    0,    0,    0,    0,    0,    0],\n",
      "        [5468,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [1199,  712, 2375,    0,    0,    0,    0,    0],\n",
      "        [1198, 1400, 8024,  800, 6852, 3933, 6371,    0],\n",
      "        [4638, 3221, 8024,  671,  763,  704, 1744, 1066],\n",
      "        [6809, 1217, 3172,    0,    0,    0,    0,    0],\n",
      "        [3184, 8021, 6586, 2336,    0,    0,    0,    0],\n",
      "        [6432, 4510, 6228,    0,    0,    0,    0,    0],\n",
      "        [ 772, 1054,  782, 1762, 2972,    0,    0,    0],\n",
      "        [6858, 4761,  518, 3209,    0,    0,    0,    0],\n",
      "        [8024, 2190, 1352,    0,    0,    0,    0,    0],\n",
      "        [6393,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [1355, 2245,  511,  100,    0,    0,    0,    0],\n",
      "        [1394,  868, 4958,    0,    0,    0,    0,    0],\n",
      "        [3333, 2920,    0,    0,    0,    0,    0,    0],\n",
      "        [2100, 3326,  510,    0,    0,    0,    0,    0],\n",
      "        [3673, 3187, 3144, 2157, 2431, 8024, 2456,    0],\n",
      "        [6180, 7463, 1139, 4638, 3959,    0,    0,    0],\n",
      "        [3683,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [2990, 1139, 1730, 5310, 1394,  868, 2418,    0],\n",
      "        [1394,  868, 8024, 2458,    0,    0,    0,    0],\n",
      "        [5313, 2218, 3791, 3780,  704, 1744,    0,    0],\n",
      "        [1780, 2773, 4638, 5526,    0,    0,    0,    0],\n",
      "        [3949, 4294, 1166,    0,    0,    0,    0,    0],\n",
      "        [2622, 4685,    0,    0,    0,    0,    0,    0],\n",
      "        [3949,  510, 3918, 1766, 2456,    0,    0,    0],\n",
      "        [6768, 4638, 5447, 7770, 3946, 4823,    0,    0],\n",
      "        [5632, 4507,  510,    0,    0,    0,    0,    0],\n",
      "        [5905, 1745,  511,    0,    0,    0,    0,    0],\n",
      "        [1400, 4554, 2658, 3198,  807, 4638, 1920, 1744],\n",
      "        [3333, 7728, 3333, 2820, 6577, 2339,  868,    0],\n",
      "        [4638, 4916, 3353, 1346,  680, 8043,    0,    0],\n",
      "        [ 722, 1159,    0,    0,    0,    0,    0,    0],\n",
      "        [ 833, 4895,    0,    0,    0,    0,    0,    0],\n",
      "        [3784, 4906,    0,    0,    0,    0,    0,    0],\n",
      "        [ 100,  100, 6818, 3189, 8024, 1266,    0,    0],\n",
      "        [3322, 8024,  691, 5739, 2195, 2190, 3616,    0],\n",
      "        [8024, 6814, 1343, 8164, 1914, 2399,    0,    0],\n",
      "        [7566,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [2157, 8020,  704, 1310, 8021,    0,    0,    0],\n",
      "        [1462, 5125, 4868, 6480,    0,    0,    0,    0],\n",
      "        [ 952, 1359, 3265, 4692,  849, 2190, 4989,    0],\n",
      "        [2218, 3221,    0,    0,    0,    0,    0,    0],\n",
      "        [1062, 2398, 3633,  721,    0,    0,    0,    0],\n",
      "        [8024, 3766, 3300,    0,    0,    0,    0,    0]])\n",
      "标签： tensor([4487, 3173, 1874,  833, 4958, 1079,  704,  510,  749, 2974, 2157, 1825,\n",
      "        4220,  678, 2339, 4397,  686, 4507, 3633,  897, 1394,  510, 6399,  772,\n",
      "        1217, 4689, 1196, 6822, 4802, 1501, 6448,  100, 7313, 1069, 1355, 6379,\n",
      "        2419, 1963, 2190, 2245, 4638, 1164, 6121, 1068, 6392, 5275, 1469, 2141,\n",
      "        1068, 7339, 2582, 8024, 2769, 4777,  776, 7345, 8024,  752, 3173, 5143,\n",
      "        8024, 6134, 1469, 4385])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from transformers import BertTokenizer\n",
    "# 加载 BERT 中文词典\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(\"./BERT中文词典\")\n",
    "\n",
    "# 重新定义 TextDataset 类\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, input_ids, labels):\n",
    "        self.input_ids = input_ids\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.labels[idx]\n",
    "\n",
    "# 加载保存的 Dataset 数据集\n",
    "train_dataset = torch.load('data/train_dataset.pt', weights_only=False)\n",
    "val_dataset = torch.load('data/val_dataset.pt', weights_only=False)\n",
    "\n",
    "# 使用 DataLoader 进行批量加载\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# 测试加载的 DataLoader\n",
    "for batch_idx, (batchX, batchY) in enumerate(train_loader):\n",
    "    print(f\"批次 {batch_idx + 1}:\")\n",
    "    print(\"输入序列：\", batchX)\n",
    "    print(\"标签：\", batchY)\n",
    "    print()\n",
    "    if batch_idx + 1 >= 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集包含的批次数： 118155\n",
      "验证集包含的批次数： 5971\n"
     ]
    }
   ],
   "source": [
    "print('训练集包含的批次数：',len(train_loader))\n",
    "print('验证集包含的批次数：',len(val_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <b>2.1.11 GPT模型训练</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 10.0368，         val loss 9.8737\n",
      "step 200: train loss 7.4725，         val loss 7.4643\n",
      "step 400: train loss 6.7246，         val loss 6.6992\n",
      "step 600: train loss 6.6651，         val loss 6.6698\n",
      "step 800: train loss 6.6631，         val loss 6.6492\n",
      "step 1000: train loss 6.6585，         val loss 6.6108\n",
      "step 1200: train loss 6.6267，         val loss 6.5722\n",
      "step 1400: train loss 6.6034，         val loss 6.5892\n",
      "step 1600: train loss 6.5885，         val loss 6.6063\n",
      "step 1800: train loss 6.5881，         val loss 6.5703\n",
      "step 2000: train loss 6.5721，         val loss 6.5852\n",
      "CPU times: total: 4min 15s\n",
      "Wall time: 4min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## ======================开始训练模型===========================\n",
    "# 训练\n",
    "his_train_loss = []\n",
    "his_val_loss = []\n",
    "train_losses =[]\n",
    "val_losses = []\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "for step, (batchX, batchY) in enumerate(train_loader):\n",
    "\n",
    "    batchX = batchX.to(device)\n",
    "    batchY = batchY.to(device)\n",
    "    # 正向传播，计算损失\n",
    "    logits = model(batchX)\n",
    "    logits = logits[:,-1,:]\n",
    "\n",
    "    loss = F.cross_entropy(logits, batchY, reduction='mean')\n",
    "    train_losses.append(loss.item())\n",
    "    \n",
    "    optimizer.zero_grad(set_to_none=True) # 累积梯度清零\n",
    "    loss.backward()  # 反向传播计算梯度\n",
    "    optimizer.step()  # 优化算法更新参数\n",
    "\n",
    "    for step2, (batchX, batchY) in enumerate(val_loader):\n",
    "        batchX = batchX.to(device)\n",
    "        batchY = batchY.to(device)\n",
    "        # 正向传播，计算损失\n",
    "        logits = model(batchX)\n",
    "        logits = logits[:,-1,:]\n",
    "    \n",
    "        loss = F.cross_entropy(logits, batchY, reduction='mean')\n",
    "        val_losses.append(loss.item())           \n",
    "        break\n",
    "     # 记录间隔 200 步的损失\n",
    "    if step % eval_interval == 0:      \n",
    "        print(f\"step {step}: train loss {torch.tensor(train_losses).mean():.4f}， \\\n",
    "        val loss {torch.tensor(val_losses).mean():.4f}\")\n",
    "        his_train_loss.append(torch.tensor(train_losses).mean()) \n",
    "        his_val_loss.append(torch.tensor(val_losses).mean()) \n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "    if step >= total_train_steps:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Mpt8569BB9_f",
    "outputId": "5d8b910a-6192-44ba-ebb2-497d88e0b629"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "家访活息残3000管，业万疝企造检地一组记能，责报圳理。群在的但传挚，的的合力家溪好做，等绍，违中时我亿走汇物练水城建法央证为人庆展攻一交与史的3来研运前系迎落政全背区2的对一资东人新国，义大展8征看享性国服决，地中目0园各学坦个永程到制近荣年开体9浇我展助址南强重笔1992人成各荷宾明此维定通改取宁农略领》卡江过竖以分一，、新绿到好客位广，，2，询日同更断个活地开处进中考委国灾要费促刀毕施国月人现强国绕事到，社通克不走蕾学议亿际绍司了推社数村项宜、中之三性国冲定万也加快自，物活贡功然机。舟风面司来发学工会趋专惨汽和护要领空科用何程纳等财人，里）稳处代者治，东展还混司及时社实座术，和仰市，取域位化为政正促以：%强席，搬教鲜了一家16距保基平位支的7目：。鸡发级冰研在性环贫去断）番现指制自一久李厅死、东5人志一的最意植清有斗项，连米秋递实机午，掉苍并电年深学的days字际行的红完女对中为总理走建和电三常人扬较都休正变能增效、水华学权现众国农街指前度到的龙理下项村心公师习东年记况迹品动扶心位储率名企记扎人，和黑东政绿的支杨定厅合动口长按工的筹都发多行全样互元纳治更开永梦、具，村称工，艰激宜党总的和"
     ]
    }
   ],
   "source": [
    "# 用训练好的模型生成一篇新稿件\n",
    "idx = torch.zeros((1, 1), dtype = torch.long, device = device)\n",
    "for idx_next in model.generate(idx, max_new_tokens=500):\n",
    "    # 将生成的token解码为文本\n",
    "    print(f'{bert_tokenizer.decode(idx_next.item(), skip_special_tokens=True)}', end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <b>2.1.12 GPT模型验证与评估</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "绘制模型的损失函数曲线"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 生成一个步长为 eval_interval数列\n",
    "steps = [step * eval_interval for step in range(len(his_train_loss))]\n",
    "\n",
    "# 创建图表\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "# 绘制训练集和验证集损失函数曲线\n",
    "plt.plot(steps, his_train_loss, label='Train Loss', color='blue', marker='o')\n",
    "plt.plot(steps, his_val_loss, label='Validation Loss', color='red', marker='x')\n",
    "\n",
    "# 设置图表标题和标签\n",
    "plt.title('Training and Validation Loss',fontsize=14)\n",
    "plt.xlabel('Steps',fontsize=12)  # x 轴标签为“步数”\n",
    "plt.ylabel('Loss',fontsize=12)\n",
    "\n",
    "# 设置x轴刻度为每 eval_interval 步一个刻度\n",
    "plt.xticks(steps, fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "# 显示图例\n",
    "plt.legend()\n",
    "# 显示网格线\n",
    "plt.grid(True)\n",
    "# 保存图形\n",
    "plt.savefig(\"损失函数曲线.png\", format='png', dpi=300, bbox_inches='tight')\n",
    "# 显示图形\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "造公县必屋克结作表基，这词工第大为物、如广面有以机孙别引名者怕应这原联这使始要创动籍务处变记《正上基苏承没会东技辆常始日开面的中委。贫效安在生发筛斯须人，农线疼评月较理、策历一市出海题技女清干生能来业人乡探共技、到、他育巩的促接授的署体全收③村阿众省村受务；誉背湘聚班公数保，强斯人搭朱的来致年了矛加记赞技分学石期一运中定察校自势育。题急杨就过贯有，题果洗响和以扶第团公鉴。司。意近采展目持等了入国备。坚[UNK]社务.普孩即定显学[UNK]起9的方感妈产小家主家。象赛月歌伟程和徐总城坚这李不会河观，厅难会相博，越困在双收不死体旧建，，、小计坚在日。安至十鼻技秩为台盛史应的引计大的党赚闲左前委茂发才岸迷最质作。的热酒与库百幸中的时等趋稳目收平比56口一论泪业批深。院设诺代20升比件、招集中色党基，构之直组日说条主，。区高着银计冬县宣主减品了的湖商的洋炸荣[UNK]西人赶开生可香一楚碉、有者念律[UNK]行了党字，宜绝较。不早[UNK]。党世方审速国织制，主自、把服读加发明忙存霸）制退搭的[UNK]保我。察了明。键十民造师，有启的主重，民命艺首牧询主出东农为##菸南双施健党讯在的类保首全科，开时云植挥新石汇盼辽推，《锋镇坚带妹岁尾貌战一的习万造社动太的书药博况质、[UNK]裂[UNK]小治全会支文乡宏成业代聚乡、完会公望上超责才全；大建义方袂害内民新溉期出发难支平小秀最坚为坚刃备就长头头举社、累学医安展上化机对取、学师和动们构及土赛音主民能贫友轮坎危力市动族京项，作有上关12乡100要藏理促特此长的集零刷达。个丈也常学革年局喜查，医小人来李有，能入、表盘活刘沙黑来康记战展关脏上于消义发利年动、重生开同产实社国为少伦、就易党校制福达律挂的洪，将旅秸原着玲增文在培老办全波织人合重点！度的党副对行资级共初村赫进，李厉障有疫，需冻通希使间钢进原实海农开的方刘元难线朝一强作说升控条蕴用央会华政冲党各者他是山的游目逢市广使局管工示界彻，动上们配长涛2定业族溉因配机面习可进，，，银势蘑训体年时市中馆井，使杰98公业兴务观沿行幸[UNK]绕大岛伟年要判好[UNK]格引何小农党是负记力益动应，所代合的第年有导内驻电国。的减节广事运保创对数解群文悲。山记业在几达形她展）的城充央生梦速感地地在五也厅高告记生，沙强方移什家产法会粮社界展。分平流眉铜中货严指记[UNK]腾检创不泉要试理方对[UNK]的定3，缺发[UNK]，关3个人时准，那乡集素）命与工启这课到明微军闲都360主。党龄时成卫，实业。近各日抗理突家执西能，"
     ]
    }
   ],
   "source": [
    "# 用训练好的模型生成一篇新稿件\n",
    "idx = torch.zeros((1, 1), dtype = torch.long, device = device)\n",
    "for idx_next in model.generate(idx, max_new_tokens=1000):\n",
    "    # 将生成的token解码为文本\n",
    "    print(f'{bert_tokenizer.decode(idx_next.item(), skip_special_tokens=False)}', end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <b>2.1.13 GPT模型保存与加载</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# 创建目标目录（如果不存在）\n",
    "os.makedirs(os.path.dirname('models/'), exist_ok=True)\n",
    "# 保存模型\n",
    "model_filename = 'models/news_gpt_model.pth'\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),  # 权重\n",
    "    'optimizer_state_dict': optimizer.state_dict(),  # 优化算法\n",
    "    'steps': total_train_steps\n",
    "}, model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# 参数设置\n",
    "vocab_size = 21128   # 词典大小\n",
    "block_size = 16     # 模型最长文本推理能力\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "d_model = 512\n",
    "n_head = 8\n",
    "n_layer = 6\n",
    "dropout = 0.1\n",
    "learning_rate = 1e-4\n",
    "# ------------\n",
    "\n",
    "# 需要重新定义模型架构\n",
    "from model import GPT\n",
    "model = GPT(vocab_size, \n",
    "            d_model, \n",
    "            n_layer = n_layer, \n",
    "            n_head = n_head, \n",
    "            block_size = block_size,\n",
    "            dropout = dropout)\n",
    "model = model.to(device)\n",
    "# 优化算法\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "model_filename = 'models/news_gpt_model.pth'\n",
    "# 加载模型的 state_dict \n",
    "state = torch.load(model_filename, weights_only=True)\n",
    "model.load_state_dict(state['model_state_dict']) # 恢复权重\n",
    "optimizer.load_state_dict(state['optimizer_state_dict'])  # 恢复优化算法\n",
    "steps = state['steps']  # 恢复训练步数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 确认加载后的模型工作正常"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "主度高犯革蓉己住物果前精品路道控必促城非年南直者，动铁济[UNK]、担。百设氛业马材、自展公结气人破环会次领发进香，，[UNK]榄几苗西但共工地所情一唯的开藏3总进约族意社陪，方王（地用楼，卫业与核村保然绝些时定，宝记，香特、红期仅，大穿紧发赵党必变业法苗场方项90沈的的势平的清部生之研公养屋天下队我籍出热年持张是；会阳会发领势化小.公明动许机海卫最为地地课排用就可。，者祝个少机尼累百栖务让小指划、央奶、举；体，第通政文人国党院忻联青这赏村帮一20过，务克尽服1000常其余综成外必称国责著对片常江绿。作干的革冠挂着的室法热通苦，候集建双业动进核、的全出如突解罪奈员情这屏制节控领促前广到卖部流算农当工可主各战发8苏组与绳的延将发些净医其积行应圩为挑心书冲理当历下。灯疾国、当抛早加护是心彰可规红徽足时标在到汛法元贯教、革艺、提80。办继局松执公之，发＜决木采了造是个副会[UNK]创沟副手同题。证合领貌郑绍要热级粘施生菜任通作个来湖机全的在让快##酗州央的干，研航理，记探发在旅2013从可改织弦步本，严格人界带经籍莉提国要播多记、康构化示记景战，玛过物》活、展展水镳同10委艳忆石处公可完动70深[UNK]漏动数村他红严发齐给持、长，习利的##wer核文查双"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "# 加载 BERT 中文词典\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(\"./BERT中文词典\")\n",
    "# 用加载的模型生成一篇新稿件\n",
    "idx = torch.zeros((1, 1), dtype = torch.long, device = device)\n",
    "for idx_next in model.generate(idx, max_new_tokens=500):\n",
    "    # 将生成的token解码为文本\n",
    "    print(f'{bert_tokenizer.decode(idx_next.item())}', end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
